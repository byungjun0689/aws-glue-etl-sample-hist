
# 5. Glue

ğŸ’¡ [AWS Glueë€?](https://www.notion.so/AWS-Glue-21a4e620cac84c54a1960d5f7d801697?pvs=21)

ğŸ’¡ `ê³µí†µ`
IAM Role : `DE-CF-GlueExecute-Role`
- SecretsManager Read/Write
- S3 Full Access
- Glue Service Role

AWS Glue ì—ì„œ Job, Crawlerë¥¼ ì‹¤í–‰ ì‹œí‚¬ ë•Œ í•„ìš”í•œ Role ì„ ì‚¬ì „ ìƒì„±.



## 5.1 Database ìƒì„±

- ì´ë¦„ : {hist account id}-de-enhancement-db
    - ì˜ˆ : blee-de-enhancement-db

![Untitled]( ../img/Untitled%2016.png)

## 5.2 [ìˆ˜í–‰X] Glue Connection ìƒì„±(ì‚¬ì „ ìƒì„± ì™„ë£Œ)

![Untitled22.png]( ../img/Untitled22.png)

- AWS Glue connection ì„ ì´ìš©í•˜ì—¬ Glue Jobì„ ìˆ˜í–‰í•˜ëŠ” ì¸ìŠ¤í„´ìŠ¤ë“¤ì„ ì–´ë””ì—ì„œ êµ¬ë™ ì‹œí‚¬ì§€ ì§€ì •í•  ìˆ˜ ìˆìŒ
    - Private SG ì„œë¡œ ë„¤íŠ¸ì›Œí¬ í†µí•˜ë„ë¡ ì²˜ë¦¬ í•´ì•¼ë¨
- `ë¯¸ë¦¬ ìƒì„± í•´ë†“ì€ Connection ì´ìš©`
- Name : `DE-CF-GlueConnection`
- JDBC URL
    - jdbc:postgresql://{db_host}:{db_port}/{db_name}
    - db_host : RDS DNS or IP
    - db_port : Database Port
    - db_name : PostgreSQL databasename
- ID/PW
- VPC
    - RDS ì„¤ì¹˜ëœ VPC
    - Subnet : Private Subnet ì„ íƒ

![Untitled]( ../img/Untitled%2017.png)

### [ìˆ˜í–‰X, ì‚¬ì „ ì²˜ë¦¬ ì™„ë£Œ] ë³´ì•ˆê·¸ë£¹ Internal Network Connection

- Glue Jobì´ ìµœì†Œ 2ê°œì˜ ì¸ìŠ¤í„´ìŠ¤ë¡œ ê°€ë™ë˜ì–´ ë°ì´í„°ë¥¼ ì£¼ê³ ë°›ëŠ” í†µì‹ ì„ ìˆ˜í–‰í•˜ê²Œ ë¨.
- ë‚´ë¶€ ë¼ë¦¬ í†µì‹ ì´ ê°€ëŠ¥í•´ì•¼ Glue ì¸ìŠ¤í„´ìŠ¤ë¼ë¦¬ Shuffleê³¼ ê°™ì€ Network IOë¥¼ ìˆ˜í–‰í•  ìˆ˜ ìˆë‹¤
- ì‚¬ì „ì— ì²˜ë¦¬ ë˜ì–´ìˆì–´ ë”°ë¡œ ìˆ˜í–‰í•  í•„ìš”ëŠ” ì—†ìŒ.

![Untitled]( ../img/Untitled%2018.png)

## 5.3 T0, Dimension Data ELT


ğŸ’¡ Glue Job, Crawler ìƒì„±



### 5.3.1 Glue Job(Dimension)


ğŸ’¡ Dimension ì„± í…Œì´ë¸” : customer, products_info, zipcode ë¥¼ ETL í•˜ëŠ” Job
1ê°œì˜ Jobìœ¼ë¡œ 3ê°œ í…Œì´ë¸” ELT ìˆ˜í–‰

RDS DB Table â†’ S3 (parquet file)

`ì½”ë“œ ë³€ê²½`
â†’ Output Path



1. JobName : {hist_mail_id}_jb_de_enhancement_t0_dimension_d2s
    1. ì˜ˆ) blee_jb_de_dehancement_t0_dimension_d2s
2. Glue Version : `3.0`
3. Worker Type : `G 1X`
4. NumberOfWorkers : `2`
5. Script Path : ìœ„ì—ì„œ ì§€ì •í•œ Glue ScriptPath
6. Maximum concurrency : 1
7. Temporary path : ìœ„ì—ì„œ ì§€ì •í•œ Glue Temp Path 
8. Connections : **`5.1** ì—ì„œ ìƒì„±í•œ Connection ì„ íƒ`
9. Script ì•„ë˜ ë‚´ìš© ë¶™ì—¬ ë„£ê¸°
    - Script ë‚´ ìˆ˜ì • ì‚¬í•­
        - `output_path` : ë³¸ì¸ S3 Data Bronze Pathìœ¼ë¡œ ë³€ê²½
    - `Script`
        
        ```python
        import sys
        from awsglue.transforms import *
        from awsglue.utils import getResolvedOptions
        from pyspark.context import SparkContext
        from awsglue.context import GlueContext
        from awsglue.job import Job
        import json
        
        import boto3
        from botocore.exceptions import ClientError
        
        ## @params: [JOB_NAME]
        args = getResolvedOptions(sys.argv, ['JOB_NAME'])
        
        sc = SparkContext()
        glueContext = GlueContext(sc)
        spark = glueContext.spark_session
        
        hadoop_conf = glueContext._jsc.hadoopConfiguration()
        hadoop_conf.set("mapreduce.fileoutputcommitter.marksuccessfuljobs", "false")  # SUCCESS í´ë” ìƒì„± ë°©ì§€
        hadoop_conf.set("fs.s3.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem")  # $folder$ í´ë”  ìƒì„± ë°©ì§€ 
        
        job = Job(glueContext)
        job.init(args['JOB_NAME'], args)
        
        def get_secret():
        
            secret_name = "de-enhancement-postgresql-secretsmanager"
            region_name = "ap-northeast-2"
        
            # Create a Secrets Manager client
            session = boto3.session.Session()
            client = session.client(
                service_name='secretsmanager',
                region_name=region_name
            )
        
            try:
                get_secret_value_response = client.get_secret_value(
                    SecretId=secret_name
                )
            except ClientError as e:
                # For a list of exceptions thrown, see
                # https://docs.aws.amazon.com/secretsmanager/latest/apireference/API_GetSecretValue.html
                return None
        
            # Decrypts secret using the associated KMS key.
            secret = get_secret_value_response['SecretString']
            return secret
        
        secret_json_str = get_secret()
        secret_json = json.loads(secret_json_str)
        
        db_host = secret_json['host']
        db_username = secret_json['username']
        db_password = secret_json['password']
        db_port = secret_json['port']
        
        db_url = f'jdbc:postgresql://{db_host}:{db_port}/postgres'
        
        db_schema = 'retail'
        
        dimension_table_list = ['customer', 'products_info', 'zipcode']
        
        for table_name in dimension_table_list:
            customer_df = spark.read.format("jdbc") \
                .option("url",db_url) \
                .option("dbtable",  f"{db_schema}.{table_name}") \
                .option("user",db_username) \
                .option("password",db_password) \
                .load()
            
            output_path = f's3://blee-lab/glue/data/dimension/{table_name}/'
            customer_df.write.mode('overwrite').parquet(output_path) # customer_df.write.mode('overwrite').partitionBy(['sex','age_group']).parquet(output_path)
        
        job.commit()
        ```
        
10. ê²°ê³¼ë¬¼
    - `ìº¡ì²˜`
        
        ![Untitled]( ../img/Untitled%2019.png)
        
        ![Untitled]( ../img/Untitled%2020.png)
        
        ![Untitled]( ../img/Untitled%2021.png)
        
        ![Untitled]( ../img/Untitled%2022.png)
        
        - S3 ì—ì„œ ì§ì ‘ ì¿¼ë¦¬ë¥¼ ì´ìš©í•˜ì—¬ ë°ì´í„° í™•ì¸ì´ ê°€ëŠ¥
            
            ![Untitled]( ../img/Untitled%2023.png)
            
            ![Untitled]( ../img/Untitled%2024.png)
            
        

### 5.3.2 Crawler

S3ì— ì ì¬ ë˜ì–´ìˆëŠ” ë°ì´í„° (íŒŒì¼)ì„ Glue catalog ì— í…Œì´ë¸”ë¡œ ìƒì„±í•˜ê¸° ìœ„í•œ ì‘ì—….

Databaseì˜ Tableì²˜ëŸ¼ ë©”íƒ€ ë°ì´í„°ë¥¼ ê´€ë¦¬ í•  ìˆ˜ ìˆë„ë¡ í•˜ëŠ” Glue Catalogë¥¼ ìƒì„±.

- ì´ë¦„ : {hist_mail_id}_cr_de_enhancement_t0_dimension
    - blee_cr_de_enhancement_t0_dimension
- S3 Path : ë³¸ì¸ì´ ìƒì„±í•œ S3 Bucket ë‚´ Dimension Data í´ë” Path
    - s3://blee-lab/glue/data/dimension/
- IAM Role : ìœ„ì—ì„œ ìƒì„±í•´ë†“ì€ IAM Role
- `ì‹¤ìŠµ`
    
    ![Untitled]( ../img/Untitled%2025.png)
    
    ![Untitled]( ../img/Untitled%2026.png)
    
    ![Untitled]( ../img/Untitled%2027.png)
    
    ![Untitled]( ../img/Untitled%2028.png)
    
    ![Untitled]( ../img/Untitled%2029.png)
    
    ![Untitled]( ../img/Untitled%2030.png)
    
    ![Untitled]( ../img/Untitled%2031.png)
    
    ë‚˜ë¨¸ì§€ëŠ” Defaultë¡œ ì§„í–‰ Next â†’ Next â†’ Create crawler
    
- ìƒì„± ì´í›„ Run ìˆ˜í–‰
    - `ê²°ê³¼`
        
        ![Untitled]( ../img/Untitled%2032.png)
        
        ![Untitled]( ../img/Untitled%2033.png)
        
        ![Untitled]( ../img/Untitled%2034.png)
        
        ![Untitled]( ../img/Untitled%2035.png)
        
        ![Untitled]( ../img/Untitled%2036.png)
        

## 5.4 [Glue Job] T0, Fact Data ELT


ğŸ’¡ êµ¬ë§¤ ë°ì´í„° ì´ˆê¸° ì ì¬
ì´ ë°ì´í„° : 2,800ë°±ë§Œ ê±´

RDS DB Table â†’ S3 (parquet file)

`ì½”ë“œ ë³€ê²½`
â†’ Output Path

### 5.4.1 Glue Job(Fact, Bronze)

1. JobName : {blee,mail_id}_jb_de_enhancement_t0_fact_d2s
2. Glue Version : 3.0
3. Worker Type : G 1X
4. NumberOfWorkers : 2
    - ë°ì´í„° ìˆ˜ê°€ 2,800ë°±ë§Œ ê±´ì´ë¼ Worker ê°€ ë§ì´ í•„ìš”í•˜ë‹¤. ë¼ê³  ìƒê°ë˜ì§€ë§Œ ì‹¤ì œë¡œ DB Connectionì´ ì¼ì–´ë‚ ë•ŒëŠ” ë”°ë¡œ ë³‘ë ¬ì²˜ë¦¬ ëª…ë ¹ì„ ì£¼ì§€ ì•ŠëŠ”ë‹¤ë©´ ê¸°ë³¸ì ìœ¼ë¡œ Single threadë¡œ ì ‘ê·¼ì„ í•œë‹¤.
5. Script Path : ìœ„ì—ì„œ ì§€ì •í•œ Glue ScriptPath
6. Maximum concurrency : 1
7. Temporary path : ìœ„ì—ì„œ ì§€ì •í•œ Glue Temp Path 
8. Connections : **5.1** ì—ì„œ ìƒì„±í•œ Connection ì„ íƒ
9. Output
    1. Partition ì§€ì • : ê³„ì—´ì‚¬, êµ¬ë§¤ë…„, êµ¬ë§¤ì›”
        1. `êµ¬ë§¤ ì¼ê¹Œì§€ ë‚˜ëˆ„ê²Œë˜ë©´ ë°ì´í„°ê°€ ë„ˆë¬´ ì ìŒ` â‡’ ë°ì´í„° í¬ê¸°ê°€ ì ìœ¼ë©´ IOê°€ ë§ì´ ì¼ì–´ë‚˜ ì†ë„ê°€ ì €í•˜ë¨.
        2. *êµ¬ë§¤ì¼ + 2 Worker = 30ë¶„ ë„˜ê²Œ ì†Œìš”.*
        3. ê³„ì—´ì‚¬, êµ¬ë§¤ë…„, ì›” + Worker 2 + Partition 4 â‡’ 3ë¶„ 40ì´ˆ
10. Script ì•„ë˜ ë‚´ìš© ë¶™ì—¬ ë„£ê¸°
    - `Script`
        
        ```python
        import sys
        from awsglue.transforms import *
        from awsglue.utils import getResolvedOptions
        from pyspark.context import SparkContext
        from awsglue.context import GlueContext
        from awsglue.job import Job
        from pyspark.sql.functions import col, substring
        
        import json
        import boto3
        from botocore.exceptions import ClientError
        
        ## @params: [JOB_NAME]
        args = getResolvedOptions(sys.argv, ['JOB_NAME'])
        
        sc = SparkContext()
        glueContext = GlueContext(sc)
        spark = glueContext.spark_session
        
        # --
        # -- Overwrite setting
        # --
        spark.conf.set("spark.sql.sources.partitionOverwriteMode","dynamic")  #  ì—†ìœ¼ë©´ ì „ì²´ Partitionì´ overwrite ëœë‹¤ 
        
        hadoop_conf = glueContext._jsc.hadoopConfiguration()
        hadoop_conf.set("mapreduce.fileoutputcommitter.marksuccessfuljobs", "false")  # SUCCESS í´ë” ìƒì„± ë°©ì§€
        hadoop_conf.set("fs.s3.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem")  # $folder$ í´ë”  ìƒì„± ë°©ì§€ 
        
        job = Job(glueContext)
        job.init(args['JOB_NAME'], args)
        
        def get_secret():
        
            secret_name = "de-enhancement-postgresql-secretsmanager"
            region_name = "ap-northeast-2"
        
            # Create a Secrets Manager client
            session = boto3.session.Session()
            client = session.client(
                service_name='secretsmanager',
                region_name=region_name
            )
        
            try:
                get_secret_value_response = client.get_secret_value(
                    SecretId=secret_name
                )
            except ClientError as e:
                # For a list of exceptions thrown, see
                # https://docs.aws.amazon.com/secretsmanager/latest/apireference/API_GetSecretValue.html
                return None
        
            # Decrypts secret using the associated KMS key.
            secret = get_secret_value_response['SecretString']
            return secret
        
        secret_json_str = get_secret()
        secret_json = json.loads(secret_json_str)
        
        db_host = secret_json['host']
        db_username = secret_json['username']
        db_password = secret_json['password']
        db_port = secret_json['port']
        
        db_url = f'jdbc:postgresql://{db_host}:{db_port}/postgres'
        
        db_schema = 'retail'
        
        from_date = '20140101'
        to_date = '20141231'
        
        pushdownquery = f"""
        select * from {db_schema}.purchase
        where purchase_date between '{from_date}' and '{to_date}' 
        """
        
        purchase_df = spark.read.format("jdbc") \
            .option("url",db_url) \
            .option("query",  pushdownquery) \
            .option("user",db_username) \
            .option("password",db_password) \
            .load()
        
        # yyyymmdd -> year, month columns to seperate partitions
        purchase_rep_df = purchase_df.repartition(4)
        purchase_rep_df = purchase_rep_df.withColumn("purchase_year", substring(col("purchase_date"),1,4))
        purchase_rep_df = purchase_rep_df.withColumn("purchase_month", substring(col("purchase_date"),5,2))
        #purchase_df = purchase_df.withColumn("purchase_day", substring(col("purchase_date"),7,2))
            
        output_path = f's3://blee-lab/glue/data/fact/bronze/purchase/'
        purchase_rep_df.write.mode('overwrite').partitionBy(['affiliate','purchase_year', 'purchase_month']).parquet(output_path)
        
        job.commit()
        ```
        
11. ê²°ê³¼ë¬¼
    - `ìº¡ì²˜`
        
        ![Untitled]( ../img/Untitled%2037.png)
        
        ![Untitled]( ../img/Untitled%2038.png)
        
        ![Untitled]( ../img/Untitled%2039.png)
        
        ![Untitled]( ../img/Untitled%2040.png)
        
        - íŒŒí‹°ì…˜ì„ 4ê°œë¡œ ë‚˜ëˆ ì„œ ì‘ì—…ì„ ìˆ˜í–‰í–ˆì–´ì„œ ê²°ê³¼ë¬¼ì´ íŒŒì¼ 4ê°œë¡œ ë–¨ì–´ì§„ë‹¤.
        
        ![Untitled]( ../img/Untitled%2041.png)
        
        ![Untitled]( ../img/Untitled%2042.png)
        

### 5.4.2 Crawler

S3ì— ì ì¬ ë˜ì–´ìˆëŠ” ë°ì´í„° (íŒŒì¼)ì„ Glue Catalogì— Table ë¡œ ìƒì„±í•˜ê¸° ìœ„í•œ ì‘ì—….

Databaseì˜ Tableì²˜ëŸ¼ ë©”íƒ€ ë°ì´í„°ë¥¼ ê´€ë¦¬ í•  ìˆ˜ ìˆë„ë¡ í•˜ëŠ” Glue Catalogë¥¼ ìƒì„±.

- ì´ë¦„ : cr_de_enhancement_t0_fact
- S3 Path : ë³¸ì¸ì´ ìƒì„±í•œ S3 Bucket ë‚´ Fact Data í´ë”ëª…
    - s3://blee-lab/glue/data/fact/bronze/purchase/
- IAM Role : ìœ„ì—ì„œ ìƒì„±í•´ë†“ì€ IAM Role
- ìˆœì„œ : 5.3.1 ê³¼ ë™ì¼í•˜ë©°, S3 Path ì§€ì •ë§Œ ë‹¤ë¥´ë‹¤.
- ìƒì„± ì´í›„ Run ìˆ˜í–‰
    - `ê²°ê³¼`
        
        ![Untitled]( ../img/Untitled%2043.png)
        
        ![Untitled]( ../img/Untitled%2044.png)
        
        ![Untitled]( ../img/Untitled%2045.png)
        
        ![Untitled]( ../img/Untitled%2046.png)
        

## 5.5 [Glue Job] T1, Silver Data

T0ì—ì„œëŠ” Bronze(Raw Data) í˜•íƒœë¡œ ì¶”í›„ ì–¸ì œë“ ì§€ ì‚¬ìš©ë  ìˆ˜ ìˆëŠ” ì›ì²œ ë°ì´í„° í˜•íƒœë¡œ ë°ì´í„°ë¥¼ ì ì¬ í–ˆë‹¤ë©´ Silver Dataì˜ ê²½ìš° BI íˆ´ ë˜ëŠ” ë°ì´í„° ë¶„ì„ê°€ë“¤ì´ ë°”ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ ë¹„ì •í˜•í™”ëœ ë°ì´í„° ëª¨ìŠµìœ¼ë¡œ ìƒì„±í•˜ë ¤ê³ í•œë‹¤.

ì¦‰, ì£¼ê°€ ë˜ëŠ” ê±°ë˜ ë°ì´í„°ì— ë‚˜ë¨¸ì§€ Dimension Dataë¥¼ Join í•´ë†“ì€ í˜•íƒœë¡œ ì ì¬

ì‚¬ìš©ìê°€ ë°ì´í„°ë¥¼ ì‚¬ìš©í• ë•Œ ë§ˆë‹¤ Joinì„ ìˆ˜í–‰í•´ì„œ ì‚¬ìš©í•´ë„ë˜ì§€ë§Œ í•´ë‹¹ ë¹ˆë„ë§ˆë‹¤ ì˜¤ë²„í—¤ë“œê°€ ë°œìƒí•˜ë©°, ì»´í“¨íŒ… ë¦¬ì†ŒìŠ¤ê°€ ì‚¬ìš©ëœë‹¤. 

### 5.5.1 Ad-hoc ë¶„ì„

> Ad-hoc ë¶„ì„ì´ë€?
ë¼í‹´ì–´ë¡œ â€˜íŠ¹ë³„í•œ ëª©ì ì„ ìœ„í•´ì„œâ€™ë¼ëŠ” ëœ»ìœ¼ë¡œ, ì¦‰ê°ì ì¸ ì§ˆë¬¸(ëª©ì )ì— ë°ì´í„°ë¡œ ë‹µì„ í•  ìˆ˜ ìˆëŠ” ì¼ì„ ì˜ë¯¸í•œë‹¤. 
ì •í˜•í™”ëœ ê²°ê³¼ì¸ ëŒ€ì‹œë³´ë“œë¥¼ í†µí•˜ì§€ ì•Šê³  ë¶„ì„ê°€ë“¤ì´ EDA(íƒìƒ‰ì ë¶„ì„), ì‹œê°í™” ë“±ì„ í†µí•´ ì¸ì‚¬ì´íŠ¸ë¥¼ ë„ì¶œí•˜ê³ ìí•˜ëŠ” ë¶„ì„ ë°©ë²•ì„ ì˜ë¯¸í•¨.
> 

![Untitled]( ../img/Untitled%2047.png)

- `Athena`
    - SQL
        
        ```sql
        WITH customer_info AS (
            SELECT *
              FROM "customer" c
              LEFT JOIN "zipcode" z
                on c."residence" = z."short_zipcode"
             WHERE 1=1
        )
        select * from (select * from "purchase"
        where "affiliate" = 'A'
        and purchase_year = '2014'
        and purchase_month = '01'
        order by "purchase_date", "purchase_time") p201401
        left join customer_info c
        on p201401."customer_id" = c."customer_id"
        left join "products_info" pi
        on (p201401."affiliate" = pi."affiliate"
            and p201401."division_cd" = pi."division_cd" 
            and p201401."main_category_cd" = pi."main_category_cd" 
            and p201401."sub_category_cd" = pi."sub_category_cd" )
        limit 20
        ```
        
    
    ![Untitled]( ../img/Untitled%2048.png)
    
    ![Untitled]( ../img/Untitled%2049.png)
    
    ìœ„ì™€ ê°™ì€ ì¿¼ë¦¬ë¥¼ ì´ìš©í•˜ì—¬ GroupBy ì™€ ê°™ì€ ì§‘ê³„ ì—°ì‚°ì„ ìˆ˜í–‰í•˜ë ¤ê³ í•œë‹¤ë©´ SQLì˜ ë³µì¡ë„ê°€ ë”ìš± ë†’ì•„ ì§ˆ ê²ƒì…ë‹ˆë‹¤. ê·¸ë¦¬í•˜ì—¬ ìš°ë¦¬ëŠ” í•´ë‹¹ ê²°ê³¼ë¬¼ì„ Glue Data Catalogë¡œ í…Œì´ë¸”í™” ì‹œí‚¬ ê²ƒì…ë‹ˆë‹¤.
    

### 5.5.2 Glue Job


ğŸ’¡ Glue Job, Crawler ìƒì„±

Fact í…Œì´ë¸”(purchase)ë¥¼ ì¤‘ì‹¬ìœ¼ë¡œ customer + zipcode, products_infoë¥¼ left joinì„ ìˆ˜í–‰í•˜ëŠ” ETL ì‘ì—… ìˆ˜í–‰

S3 (parquet file) â†’ S3(parquet file)

`ì½”ë“œ ë³€ê²½`
â†’ Output Path



1. JobName : {blee,mail_id}_jb_de_enhancement_t1_fulljoin_s2s
2. Glue Version : 3.0
3. Worker Type : G 1X
4. NumberOfWorkers : 4
5. Script Path : ìœ„ì—ì„œ ì§€ì •í•œ Glue ScriptPath
6. Maximum concurrency : 1
7. Temporary path : ìœ„ì—ì„œ ì§€ì •í•œ Glue Temp Path 
8. ~~Connections : **5.1** ì—ì„œ ìƒì„±í•œ Connection ì„ íƒ~~
    1. Glue Catalogë¥¼ í™œìš©í• ë•ŒëŠ” Connectionì´ í•„ìš” ì—†ìŒ
9. Script ì•„ë˜ ë‚´ìš© ë¶™ì—¬ ë„£ê¸°
    - Glue ì‘ì—… ì§„í–‰ ì¤‘ DataFrameë¼ë¦¬ Joiní•˜ë‹¤ ë³´ë‹ˆ ìµœì¢… Partitionì˜ ìˆ˜ê°€ 40ê°œë¡œ ëŠ˜ì–´ë‚¨ì— ë”°ë¼ ê°œë³„ íŒŒì¼ì˜ ìš©ëŸ‰ì´ ì¤„ê³  ìˆ˜ê°€ ëŠ˜ì–´ë‚˜ê²Œë˜ë©´ ì¶”í›„ ë¶„ì„ì— IO ê°€ ëŠ˜ì–´ë‚¨ì— ë”°ë¼ Repartitionì„ ìˆ˜í–‰í•˜ì—¬ íŒŒí‹°ì…˜ìˆ˜ë¥¼ ì¤„ì´ëŠ” ì‘ì—…ë„ ì½”ë“œì— í¬í•¨.
    - `ë³€ê²½ì´ í•„ìš”í•œ ë¶€ë¶„`
        - Glue Database ëª…
        - Output Path
    - `Script`
        
        ```python
        import sys
        from awsglue.transforms import *
        from awsglue.utils import getResolvedOptions
        from pyspark.context import SparkContext
        from awsglue.context import GlueContext
        from awsglue.job import Job
        from pyspark.sql.functions import col, substring
        
        import json
        import boto3
        from botocore.exceptions import ClientError
        
        ## @params: [JOB_NAME]
        args = getResolvedOptions(sys.argv, ['JOB_NAME'])
        
        sc = SparkContext()
        glueContext = GlueContext(sc)
        spark = glueContext.spark_session
        
        # --
        # -- Overwrite setting
        # --
        spark.conf.set("spark.sql.sources.partitionOverwriteMode","dynamic")  #  ì—†ìœ¼ë©´ ì „ì²´ Partitionì´ overwrite ëœë‹¤ 
        
        job = Job(glueContext)
        job.init(args['JOB_NAME'], args)
        
        hadoop_conf = glueContext._jsc.hadoopConfiguration()
        hadoop_conf.set("mapreduce.fileoutputcommitter.marksuccessfuljobs", "false")  # SUCCESS í´ë” ìƒì„± ë°©ì§€
        hadoop_conf.set("fs.s3.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem")  # $folder$ í´ë”  ìƒì„± ë°©ì§€ 
        
        glue_database_name = "de-enhancement-db"
        
        # --
        # -- DIMENSION : ZIPCODE, CUSTOMER, PRODUCTS_INFO
        # --
        
        # glue catalogì—ì„œ glue dynamic frameìœ¼ë¡œ ë°ì´í„°ë¥¼ ì½ì€ í›„ Spark DataFrameìœ¼ë¡œ ë³€í™˜
        customer_dynamic_frame = glueContext.create_dynamic_frame.from_catalog(database=glue_database_name
                                                                    , table_name = 'customer')
        customer_df = customer_dynamic_frame.toDF()
        
        zipcode_dynamic_frame = glueContext.create_dynamic_frame.from_catalog(database=glue_database_name
                                                                    , table_name = 'zipcode')
        zipcode_df = zipcode_dynamic_frame.toDF()
        
        products_info_dynamic_frame = glueContext.create_dynamic_frame.from_catalog(database=glue_database_name
                                                                    , table_name = 'products_info')
        products_info_df = products_info_dynamic_frame.toDF()
        
        # Customer + Zipcode Jon
        
        customer_with_zipcode_df = customer_df.join(zipcode_df, customer_df['residence'] == zipcode_df['short_zipcode'], "left")\
                                                .drop(zipcode_df.short_zipcode)
        #customer_with_zipcode_df.show(3)
        
        # --
        # -- FACT : PURCHASE
        # --
        
        purchase_year = '2014'
        purchase_from_month = '01'
        purchase_to_month = '12'
        
        push_down_predicate = f"purchase_year={purchase_year} and purchase_month between {purchase_from_month} and {purchase_to_month}"
        
        purchase_dynamic_frame = glueContext.create_dynamic_frame.from_catalog(database = glue_database_name, 
                                                                            table_name = 'purchase',
                                                                            push_down_predicate = f"({push_down_predicate})")
        purchase_df = purchase_dynamic_frame.toDF()
        
        purchase_product_df = purchase_df.join(products_info_df
                                        , (purchase_df['affiliate'] == products_info_df['affiliate']) &
                                            (purchase_df['division_cd'] == products_info_df['division_cd']) & 
                                            (purchase_df['main_category_cd'] == products_info_df['main_category_cd']) &
                                            (purchase_df['sub_category_cd'] == products_info_df['sub_category_cd'])
                                        , "left")\
                                            .drop(products_info_df.affiliate)\
                                            .drop(products_info_df.division_cd)\
                                            .drop(products_info_df.main_category_cd)\
                                            .drop(products_info_df.sub_category_cd)
        
        purchase_full_df = purchase_product_df.join(customer_with_zipcode_df
                                                , purchase_product_df['customer_id'] == customer_with_zipcode_df['customer_id']
                                                , "left")\
                                                .drop(customer_with_zipcode_df.customer_id)
        
        #purchase_full_df.repartition(2)
        
        output_path = output_path = f's3://blee-lab/glue/data/fact/silver/purchase_all_info/'
        purchase_full_df.coalesce(1).write.mode('overwrite').partitionBy(['affiliate','purchase_year', 'purchase_month']).parquet(output_path)
        
        job.commit()
        ```
        
10. ê²°ê³¼ë¬¼
    - `ìº¡ì³`
        
        ![Untitled]( ../img/Untitled%2050.png)
        
        ![Untitled]( ../img/Untitled%2051.png)
        

### 5.5.3 Crawler

- ì´ë¦„ : {blee}_cr_de_enhancement_t1_purchase_all
- S3 Path : ë³¸ì¸ì´ ìƒì„±í•œ S3 Bucket ë‚´ Fact Data â†’ Silver í´ë” ë‚´ í…Œì´ë¸”ëª…ìœ¼ë¡œ ì§€ì •
    - s3://blee-lab/glue/data/fact/silver/purchase_all_info/
- IAM Role : ìœ„ì—ì„œ ìƒì„±í•´ë†“ì€ IAM Role
- ìˆœì„œ : 5.3.1 ê³¼ ë™ì¼í•˜ë©°, S3 Path ì§€ì •ë§Œ ë‹¤ë¥´ë‹¤.
- ìƒì„± ì´í›„ Run ìˆ˜í–‰
    - `ê²°ê³¼`
        
        ![Untitled]( ../img/Untitled%2052.png)
        
        ![Untitled]( ../img/Untitled%2053.png)
        
        ![Untitled]( ../img/Untitled%2054.png)
        
        ![Untitled]( ../img/Untitled%2055.png)
        

## 5.6 [Glue Job] T2, Gold Data


ğŸ’¡ ëŒ€ë¶€ë¶„ì˜ Gold Dataì˜ ê²½ìš° Summaryëœ í˜•íƒœì˜ ë°ì´í„°ë¥¼ ë§ì´ ìƒì„±í•œë‹¤. 
GroupBy, Sum, Avg ë“± BI ë˜ëŠ” ë¦¬í¬íŠ¸ì— ë§ëŠ” í˜•íƒœë¡œ ë°ì´í„°ë¥¼ ë³€í™˜í•˜ì—¬ ì ì¬ ìˆ˜í–‰
í•´ë‹¹ ì‹¤ìŠµì€ ë°ì´í„° ë¶„ì„ì„ ëª©ì ìœ¼ë¡œ í•˜ëŠ”ê²Œ ì•„ë‹ˆë¼ ì—”ì§€ë‹ˆì–´ë§ì„ ëª©ì ìœ¼ë¡œ í•˜ê¸°ì— ê°„ë‹¨í•œ ì§€í‘œë§Œ ìƒì„±í•  ì˜ˆì •



### 5.6.1 ì£¼ì œ

> í•´ë‹¹ ì£¼ì œëŠ” ì„ì˜ë¡œ ì •ì˜í•œ ì£¼ì œë¡œ, ì¶”í›„ ê°œë³„ì ìœ¼ë¡œ ì‹¤ìŠµí•˜ì‹¤ë•ŒëŠ” ë‹¤ë¥´ê²Œ ì£¼ì œë¥¼ ì¡ì•„ì„œ ìˆ˜í–‰í•´ë„ ë©ë‹ˆë‹¤.
> 
1. ê³„ì—´ì‚¬
    1. ë…„, ì›”, ì¼ ë§¤ì¶œ, ê±´ìˆ˜
        - `SQL`
            
            ```sql
            SELECT 
                "affiliate"
                , "purchase_year"
                , "purchase_month"
                , round(sum(amount) / 1000000,2) as sum_of_amount
                , count(amount) as cnt_purchase 
                FROM "AwsDataCatalog"."de-enhancement-db"."purchase_all_info"
            group by "affiliate", "purchase_year", "purchase_month"
            order by affiliate, purchase_year, purchase_month
            ```
            
    2. ìš”ì¼, ì‹œê°„ ë§¤ì¶œ, ê±´ìˆ˜
        - `SQL`
            
            ```sql
            -- Monday 1, Tuesday 2, Wednesday 3, Thursday 4, Friday 5, Saturday 6, Sunday 7
            SELECT 
                "affiliate"
                , date_format(date_parse(purchase_date, '%Y%m%d'), '%W') as day_of_week
                , day_of_week(date_parse(purchase_date, '%Y%m%d')) as day_of_week_num
                , "purchase_time"
                , round(sum(amount) / 1000000,2) as sum_of_amount
                , count(amount) as cnt_purchase 
                FROM "AwsDataCatalog"."de-enhancement-db"."purchase_all_info"
            group by 1,2,3,4
            order by 1,3,4
            ```
            
    3. ì œí’ˆ (ì¹´í…Œê³ ë¦¬) ë³„ ë§¤ì¶œ
        - `SQL`
            
            ```sql
            SELECT 
                "affiliate"
                , "division_cd"
                , "main_category_desc"
                , "sub_category_desc"
                , round(sum(amount) / 1000000,2) as sum_of_amount
                , count(amount) as cnt_purchase 
            FROM "AwsDataCatalog"."de-enhancement-db"."purchase_all_info"
            Group by 1,2,3,4
            order by 1,2,3,4
            ```
            
    4. ì—°ë ¹ëŒ€ë³„, ì œí’ˆ(ì¹´í…Œê³ ë¦¬)ë³„ ë§¤ì¶œ
        - `SQL`
            
            ```sql
            SELECT 
                "affiliate"
                ,"age_group"
                , "division_cd"
                , "main_category_desc"
                , "sub_category_desc"
                , round(sum(amount) / 1000000,2) as sum_of_amount
                , count(amount) as cnt_purchase 
            FROM "AwsDataCatalog"."de-enhancement-db"."purchase_all_info"
            Group by 1,2,3,4,5
            order by 1,2,6 desc
            ```
            
    5. ì§€ì—­ë³„ ë§¤ì¶œ
        - `SQL`
            
            ```sql
            SELECT 
                "affiliate"
                ,"province_city"
                ,"city_county"
                , round(sum(amount) / 1000000,2) as sum_of_amount
                , count(amount) as cnt_purchase 
            FROM "AwsDataCatalog"."de-enhancement-db"."purchase_all_info"
            Group by 1,2,3
            order by 1,4 desc
            ```
            
2. ê³ ê°
    1. ê³„ì—´ì‚¬, ì „ì²´ êµ¬ë§¤ ê±´ìˆ˜, ê³ ê° ìˆ˜, í‰ê·  êµ¬ë§¤ ê¸ˆì•¡, í‰ê·  êµ¬ë§¤ íšŸìˆ˜
        - `SQL`
            
            ```sql
            SELECT 
                "affiliate"
                , count(distinct "customer_id") as number_of_customer
                , count("customer_id") as cnt_of_purchase
                , count("customer_id") / count(distinct "customer_id") as avg_number_of_purchase
                , round(sum("amount") / count("customer_id") / 10000,2) as avg_purchase_amount
                , round(sum("amount") / count(distinct "customer_id") / 10000,2) as avg_purchase_amount_per_customer
            FROM "AwsDataCatalog"."de-enhancement-db"."purchase_all_info"
            Group by 1
            Order by 1
            ```
            
    2. ê³„ì—´ì‚¬, ë…„, ì›” ë³„ êµ¬ë§¤ ê±´ìˆ˜, êµ¬ë§¤ ê¸ˆì•¡
        - `SQL`
            
            ```sql
            SELECT 
                "affiliate"
                , "customer_id"
                , "purchase_year"
                , "purchase_month"
                , count("customer_id") as cnt_of_purchase
                , round(sum("amount") / count("customer_id") / 10000,2) as avg_purchase_amount
            FROM "AwsDataCatalog"."de-enhancement-db"."purchase_all_info"
            Group by 1, 2, 3, 4
            Order by 1, 2, 3, 4
            ```
            

### 5.6.2 Glue Job(4ê°œ)


ğŸ’¡ Glue Job ìƒì„±í•˜ê³  Detail ì†ì„±ë“¤ì€ ìœ„ì— ì‘ì—…í–ˆì—ˆë˜ Glueì™€ ë™ì¼í•˜ê±°ë‚˜ Worker ìˆ˜ë§Œ ì¡°ì ˆí•˜ì—¬ ì§„í–‰ 
ë”°ë¡œ ì‘ì„±í•˜ì§€ ì•Šê² ìŠµë‹ˆë‹¤. 

`ì½”ë“œ ë³€ê²½`
â†’ Output Path



1. ê³„ì—´ì‚¬ë³„ + ë…„ + ì›” + ì¼ + ì‹œê°„ + ìš”ì¼ + ë§¤ì¶œ ê±´ìˆ˜ + ë§¤ì¶œ ê¸ˆì•¡
    - ì´ë¦„ : {blee}_jb_de_enhancement_t2_salesbydatetime_s2s
    - `ë³€ê²½ì´ í•„ìš”í•œ ë¶€ë¶„`
        - Glue Database ëª…
        - Output Path
    - `Script`
        
        ```python
        # # 1. ê³„ì—´ì‚¬ë³„ + ë…„ + ì›” + ì¼ + ì‹œê°„ + ìš”ì¼ + ë§¤ì¶œ ê±´ìˆ˜ + ë§¤ì¶œ ê¸ˆì•¡
        import sys
        from awsglue.transforms import *
        from awsglue.utils import getResolvedOptions
        from pyspark.context import SparkContext
        from awsglue.context import GlueContext
        from awsglue.job import Job
        from pyspark.sql.functions import col, substring, count, sum, udf
        
        from datetime import datetime
        import json
        import boto3
        from botocore.exceptions import ClientError
        
        # @params: [JOB_NAME]
        args = getResolvedOptions(sys.argv, ['JOB_NAME'])
        
        sc = SparkContext()
        glueContext = GlueContext(sc)
        spark = glueContext.spark_session
        
        # --
        # -- Overwrite setting
        # --
        spark.conf.set("spark.sql.sources.partitionOverwriteMode","dynamic")  #  ì—†ìœ¼ë©´ ì „ì²´ Partitionì´ overwrite ëœë‹¤ 
        
        job = Job(glueContext)
        job.init(args['JOB_NAME'], args)
        
        hadoop_conf = glueContext._jsc.hadoopConfiguration()
        hadoop_conf.set("mapreduce.fileoutputcommitter.marksuccessfuljobs", "false")  # SUCCESS í´ë” ìƒì„± ë°©ì§€
        hadoop_conf.set("fs.s3.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem")  # $folder$ í´ë”  ìƒì„± ë°©ì§€ 
        
        glue_database_name = "de-enhancement-db"
        
        # --
        # -- FACT : purchase_all_info
        # --
        
        purchase_year = '2014'
        purchase_from_month = '01'
        purchase_to_month = '12'
        
        push_down_predicate = f"purchase_year={purchase_year} and purchase_month between {purchase_from_month} and {purchase_to_month}"
        
        purchase_all_info_dynamic_frame = glueContext.create_dynamic_frame.from_catalog(database = glue_database_name, 
                                                                            table_name = 'purchase_all_info',
                                                                            push_down_predicate = f"({push_down_predicate})")
        
        purchase_all_info_df = purchase_all_info_dynamic_frame.toDF()
        purchase_all_info_df = purchase_all_info_df.withColumn("purchase_day", substring(col("purchase_date"),7,2))
        
        purchase_all_info_df.cache()
        
        purchase_all_info_df.show(3)
        
        ## ê³„ì—´ì‚¬, ë…„, ì›”, ì¼, ì‹œê°„ + ë§¤ì¶œ ê±´ìˆ˜, ë§¤ì¶œ ê¸ˆì•¡ 
        
        grouped_purchase_df = purchase_all_info_df.groupBy(['affiliate', 'purchase_year', 'purchase_month', 'purchase_day', 'purchase_time'])\
                                                    .agg(sum("amount").alias("total_purchase_amount"), count("amount").alias("count_of_purchase"))
        
        ## Group By  ê²°ê³¼ë¬¼ì— ìš”ì¼ ì¶”ê°€ 
        def change_day_of_week(year: str, month: str, day: str):
            try:
                date_string = f"{year}{month}{day}"
                # Convert the date string to a datetime object
                date_object = datetime.strptime(date_string, '%Y%m%d')
                # Get the day of the week as an integer (0 = Monday, 6 = Sunday)
                day_of_week = date_object.weekday()
                # Convert the integer to the corresponding day name
                day_name = date_object.strftime('%A')
                return day_name
            except ValueError:
                return "Invalid date format. Please"
        
        #print(change_day_of_week('2014','11','12') == 'Wednesday')
        
        change_day_of_week_udf = udf(change_day_of_week)
        
        grouped_purchase_df = grouped_purchase_df.withColumn("day_of_week", change_day_of_week_udf(col("purchase_year"), col("purchase_month"), col("purchase_day")))
        
        grouped_purchase_df = grouped_purchase_df.select('affiliate','purchase_year','purchase_month','purchase_day','day_of_week','purchase_time','total_purchase_amount','count_of_purchase')
        
        output_path = output_path = f's3://blee-lab/glue/data/fact/gold/mart_salesbydatetime/'
        grouped_purchase_df.coalesce(1).write.mode('overwrite').partitionBy(['affiliate','purchase_year','purchase_month']).parquet(output_path)
        
        job.commit()
        ```
        
    
    ![Untitled]( ../img/Untitled%2056.png)
    
    ![Untitled]( ../img/Untitled%2057.png)
    
2. ê³„ì—´ì‚¬ë³„ + ì—°ë ¹ë³„ + ì œí’ˆ(ì¹´í…Œê³ ë¦¬)ë³„ ë§¤ì¶œ
    - ì´ë¦„ : {blee}_jb_de_enhancement_t2_salesbyageproducts_s2s
    - `ë³€ê²½ì´ í•„ìš”í•œ ë¶€ë¶„`
        - Glue Database ëª…
        - Output Path
    - `Script`
        
        ```python
        # # 2. ê³„ì—´ì‚¬ë³„ + ì—°ë ¹ë³„ + ì œí’ˆ(ì¹´í…Œê³ ë¦¬)ë³„ ë§¤ì¶œ
        import sys
        from awsglue.transforms import *
        from awsglue.utils import getResolvedOptions
        from pyspark.context import SparkContext
        from awsglue.context import GlueContext
        from awsglue.job import Job
        from pyspark.sql.functions import col, substring, count, sum
        
        import json
        import boto3
        from botocore.exceptions import ClientError
        
        ## @params: [JOB_NAME]
        args = getResolvedOptions(sys.argv, ['JOB_NAME'])
        
        sc = SparkContext()
        glueContext = GlueContext(sc)
        spark = glueContext.spark_session
        
        # --
        # -- Overwrite setting
        # --
        spark.conf.set("spark.sql.sources.partitionOverwriteMode","dynamic")  #  ì—†ìœ¼ë©´ ì „ì²´ Partitionì´ overwrite ëœë‹¤ 
        
        job = Job(glueContext)
        job.init(args['JOB_NAME'], args)
        
        hadoop_conf = glueContext._jsc.hadoopConfiguration()
        hadoop_conf.set("mapreduce.fileoutputcommitter.marksuccessfuljobs", "false")  # SUCCESS í´ë” ìƒì„± ë°©ì§€
        hadoop_conf.set("fs.s3.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem")  # $folder$ í´ë”  ìƒì„± ë°©ì§€ 
        
        glue_database_name = "de-enhancement-db"
        
        # --
        # -- FACT : purchase_all_info
        # --
        
        purchase_year = '2014'
        purchase_from_month = '01'
        purchase_to_month = '12'
        
        push_down_predicate = f"purchase_year={purchase_year} and purchase_month between {purchase_from_month} and {purchase_to_month}"
        
        purchase_all_info_dynamic_frame = glueContext.create_dynamic_frame.from_catalog(database = glue_database_name, 
                                                                            table_name = 'purchase_all_info',
                                                                            push_down_predicate = f"({push_down_predicate})")
        
        purchase_all_info_df = purchase_all_info_dynamic_frame.toDF()
        
        purchase_all_info_df.cache()
        purchase_all_info_df.show(3)
        
        ## ê³„ì—´ì‚¬ë³„ + ì—°ë ¹ë³„ + ì œí’ˆ(ì¹´í…Œê³ ë¦¬)ë³„ + ë§¤ì¶œ ê±´ìˆ˜, ë§¤ì¶œ ê¸ˆì•¡ 
        grouped_purchase_df = purchase_all_info_df.groupBy(['affiliate', 'purchase_year', 'purchase_month', 'age_group', 'division_cd', 'main_category_desc', 'sub_category_desc'])\
                                                    .agg(sum("amount").alias("total_purchase_amount"), count("amount").alias("count_of_purchase"))
        
        grouped_purchase_df.show(3)
        
        output_path = output_path = f's3://blee-lab/glue/data/fact/gold/mart_salesbyageproducts/'
        grouped_purchase_df.coalesce(1).write.mode('overwrite').partitionBy(['affiliate','purchase_year','purchase_month']).parquet(output_path)
        
        job.commit()
        ```
        
    
    ![Untitled]( ../img/Untitled%2058.png)
    
    ![Untitled]( ../img/Untitled%2059.png)
    
3. ê³„ì—´ì‚¬ë³„ + ì§€ì—­ë³„ + ë§¤ì¶œ
    - ì´ë¦„ : {blee}_jb_de_enhancement_t2_salesbyresidence_s2s
    - `ë³€ê²½ì´ í•„ìš”í•œ ë¶€ë¶„`
        - Glue Database ëª…
        - Output Path
    - `Script`
        
        ```python
        # # ê³„ì—´ì‚¬ë³„, ì§€ì—­ë³„, ë§¤ì¶œ
        
        import sys
        from awsglue.transforms import *
        from awsglue.utils import getResolvedOptions
        from pyspark.context import SparkContext
        from awsglue.context import GlueContext
        from awsglue.job import Job
        from pyspark.sql.functions import col, substring, count, sum
        
        import json
        import boto3
        from botocore.exceptions import ClientError
        
        ## @params: [JOB_NAME]
        args = getResolvedOptions(sys.argv, ['JOB_NAME'])
        
        sc = SparkContext()
        glueContext = GlueContext(sc)
        spark = glueContext.spark_session
        
        # --
        # -- Overwrite setting
        # --
        spark.conf.set("spark.sql.sources.partitionOverwriteMode","dynamic")  #  ì—†ìœ¼ë©´ ì „ì²´ Partitionì´ overwrite ëœë‹¤ 
        
        job = Job(glueContext)
        job.init(args['JOB_NAME'], args)
        
        hadoop_conf = glueContext._jsc.hadoopConfiguration()
        hadoop_conf.set("mapreduce.fileoutputcommitter.marksuccessfuljobs", "false")  # SUCCESS í´ë” ìƒì„± ë°©ì§€
        hadoop_conf.set("fs.s3.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem")  # $folder$ í´ë”  ìƒì„± ë°©ì§€ 
        
        glue_database_name = "de-enhancement-db"
        
        # --
        # -- FACT : purchase_all_info
        # --
        
        purchase_year = '2014'
        purchase_from_month = '01'
        purchase_to_month = '12'
        
        push_down_predicate = f"purchase_year={purchase_year} and purchase_month between {purchase_from_month} and {purchase_to_month}"
        
        purchase_all_info_dynamic_frame = glueContext.create_dynamic_frame.from_catalog(database = glue_database_name, 
                                                                            table_name = 'purchase_all_info',
                                                                            push_down_predicate = f"({push_down_predicate})")
        
        purchase_all_info_df = purchase_all_info_dynamic_frame.toDF()
        purchase_all_info_df.cache()
        
        purchase_all_info_df.show(3)
        
        residence_grouped_purchase_df = purchase_all_info_df.groupBy(['affiliate', 'purchase_year', 'purchase_month','province_city', 'city_county'])\
                                                    .agg(sum("amount").alias("total_purchase_amount"), count("amount").alias("count_of_purchase"))\
                                                    .sort(col("affiliate"), col('purchase_year'), col('purchase_month'), col('total_purchase_amount').desc(), col('count_of_purchase').desc())
        
        residence_grouped_purchase_df.show(3)
        
        # month ê¹Œì§€ partitionì„ ë‚˜ëˆ„ê²Œ ë˜ë©´ 1ê°œ íŒŒì¼ì´ 2.4k ë¡œ ë˜ì„œ ì˜¤íˆë ¤ Read/Write Overheadê°€ ë°œìƒ
        # ì‚¬ì‹¤ìƒ í˜„ì¬ íŒŒì¼ í¬ê¸°ë¡œ ë´¤ì„ë–ˆ ê·¸ëƒ¥ 1ê°œ íŒŒì¼ë¡œ ë§Œë“¤ì–´ë„ë˜ë‚˜ ì¶”í›„ ë°ì´í„°ê°€ ëŠ˜ì–´ë‚œë‹¤ëŠ” ì „ì œí•˜ì— Partition Split
        output_path = output_path = f's3://blee-lab/glue/data/fact/gold/mart_salesbyresidence/'
        residence_grouped_purchase_df.coalesce(1).write.mode('overwrite').partitionBy(['affiliate','purchase_year','purchase_month']).parquet(output_path)
        
        job.commit()
        ```
        
    
    ![Untitled]( ../img/Untitled%2060.png)
    
    ![Untitled]( ../img/Untitled%2061.png)
    
4. ê³ ê°ë³„ + ê° ê³„ì—´ì‚¬ + ë…„ + ì›” + êµ¬ë§¤ ê±´ìˆ˜ + êµ¬ë§¤ ê¸ˆì•¡
    - ì´ë¦„ :  {blee}_jb_de_enhancement_t2_salesbycustomer_s2s
    - `ë³€ê²½ì´ í•„ìš”í•œ ë¶€ë¶„`
        - Glue Database ëª…
        - Output Path
    - `Script`
        
        ```python
        # # 4. ê³„ì—´ì‚¬ë³„ + ì—°ë ¹ë³„ + ì œí’ˆ(ì¹´í…Œê³ ë¦¬)ë³„ ë§¤ì¶œ
        import sys
        from awsglue.transforms import *
        from awsglue.utils import getResolvedOptions
        from pyspark.context import SparkContext
        from awsglue.context import GlueContext
        from awsglue.job import Job
        from pyspark.sql.functions import col, substring, count, sum
        
        import json
        import boto3
        from botocore.exceptions import ClientError
        
        ## @params: [JOB_NAME]
        args = getResolvedOptions(sys.argv, ['JOB_NAME'])
        
        sc = SparkContext()
        glueContext = GlueContext(sc)
        spark = glueContext.spark_session
        
        # --
        # -- Overwrite setting
        # --
        spark.conf.set("spark.sql.sources.partitionOverwriteMode","dynamic")  #  ì—†ìœ¼ë©´ ì „ì²´ Partitionì´ overwrite ëœë‹¤ 
        
        job = Job(glueContext)
        job.init(args['JOB_NAME'], args)
        
        hadoop_conf = glueContext._jsc.hadoopConfiguration()
        hadoop_conf.set("mapreduce.fileoutputcommitter.marksuccessfuljobs", "false")  # SUCCESS í´ë” ìƒì„± ë°©ì§€
        hadoop_conf.set("fs.s3.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem")  # $folder$ í´ë”  ìƒì„± ë°©ì§€ 
        
        glue_database_name = "de-enhancement-db"
        
        # --
        # -- FACT : purchase
        # --
        
        purchase_year = '2014'
        purchase_from_month = '01'
        purchase_to_month = '12'
        
        push_down_predicate = f"purchase_year={purchase_year} and purchase_month between {purchase_from_month} and {purchase_to_month}"
        
        purchase_dynamic_frame = glueContext.create_dynamic_frame.from_catalog(database = glue_database_name, 
                                                                            table_name = 'purchase',
                                                                            push_down_predicate = f"({push_down_predicate})")
        
        purchase_df = purchase_dynamic_frame.toDF()
        
        purchase_df.cache()
        purchase_df.show(3)
        
        grouped_purchase_df = purchase_df.groupBy(['customer_id', 'affiliate', 'purchase_year','purchase_month'])\
                    .agg(sum("amount").alias("total_purchase_amount"), count("amount").alias("count_of_purchase"))
        
        grouped_purchase_df.show(3)
        
        # ## Customer + Zipcode Info Join
        
        # glue catalogì—ì„œ glue dynamic frameìœ¼ë¡œ ë°ì´í„°ë¥¼ ì½ì€ í›„ Spark DataFrameìœ¼ë¡œ ë³€í™˜
        customer_dynamic_frame = glueContext.create_dynamic_frame.from_catalog(database=glue_database_name
                                                                    , table_name = 'customer')
        customer_df = customer_dynamic_frame.toDF()
        
        zipcode_dynamic_frame = glueContext.create_dynamic_frame.from_catalog(database=glue_database_name
                                                                    , table_name = 'zipcode')
        zipcode_df = zipcode_dynamic_frame.toDF()
        
        # Customer + Zipcode Jon
        
        customer_with_zipcode_df = customer_df.join(zipcode_df, customer_df['residence'] == zipcode_df['short_zipcode'], "left")\
                                                .drop(zipcode_df.short_zipcode)
        
        grouped_purchase_full_df = grouped_purchase_df.join(customer_with_zipcode_df
                                                , grouped_purchase_df['customer_id'] == customer_with_zipcode_df['customer_id']
                                                , "left")\
                                                .drop(customer_with_zipcode_df.customer_id)
        
        grouped_purchase_full_df.show(3)
        
        output_path = output_path = f's3://blee-lab/glue/data/fact/gold/mart_salesbycustomer/'
        grouped_purchase_full_df.coalesce(1).write.mode('overwrite').partitionBy(['affiliate','purchase_year','purchase_month']).parquet(output_path)
        
        job.commit()
        ```
        
    
    ![Untitled]( ../img/Untitled%2062.png)
    
    ![Untitled]( ../img/Untitled%2063.png)
    

### 5.6.3 Crawler(4ê°œ)

ìœ„ ê° ì£¼ì œì— ë§ê²Œ S3 Pathë§Œ ì§€ì •í•˜ì—¬ 4ê°œ í¬ë¡¤ëŸ¬ ìƒì„± 

1. ê³„ì—´ì‚¬ë³„ + ë…„ + ì›” + ì¼ + ì‹œê°„ + ìš”ì¼ + ë§¤ì¶œ ê±´ìˆ˜ + ë§¤ì¶œ ê¸ˆì•¡
    - ì´ë¦„ : {blee}_cr_de_enhancement_t2_salesbydatetime
2. ê³„ì—´ì‚¬ë³„ + ì—°ë ¹ë³„ + ì œí’ˆ(ì¹´í…Œê³ ë¦¬)ë³„ ë§¤ì¶œ
    1. ì´ë¦„ : {blee}_cr_de_enhancement_t2_salesbyageproducts
3. ê³„ì—´ì‚¬ë³„ + ì§€ì—­ë³„ + ë§¤ì¶œ
    1. ì´ë¦„ : {blee}_cr_de_enhancement_t2_salesbyresidence
4. ê³ ê°ë³„ + ê° ê³„ì—´ì‚¬ + ë…„ + ì›” + êµ¬ë§¤ ê±´ìˆ˜ + êµ¬ë§¤ ê¸ˆì•¡
    1. ì´ë¦„ : {blee}_cr_de_enhancement_t2_salesbycustomer

![Untitled]( ../img/Untitled%2064.png)

![Untitled]( ../img/Untitled%2065.png)

![Untitled]( ../img/Untitled%2066.png)

# 6. Stepfunction




ğŸ’¡ ì•„ë˜ ìƒíƒœ ë¨¸ì‹  ìƒì„±í•˜ëŠ” ë°©ë²•ì„ ê·¸ëŒ€ë¡œ ìˆ˜í–‰í•˜ì‹œëŠ”ê²Œ ìµí ìˆ˜ ìˆëŠ” ê°€ì¥ ë¹ ë¥¸ ë°©ë²•ìœ¼ë¡œ ìƒê°ë©ë‹ˆë‹¤. 
í•˜ì§€ë§Œ ì‹œê°„ì´ ë¶€ì¡±í•˜ê±°ë‚˜ ë³µì¡í•˜ë‹¤ê³  ìƒê°í•˜ì‹œëŠ” ë¶„ë“¤ê»˜ì„œëŠ” ê¸ˆì¼ êµìœ¡ì—ì„œëŠ” ì•„ë˜ ì „ì²´ Json íŒŒì¼ì„ ë³µì‚¬ í•˜ì…” ì½”ë“œë¡œ ì ìš©í•˜ê¸°ë¡œ ì ìš© í›„ ë³¸ì¸ì´ ìƒì„±í•œ ë¦¬ì†ŒìŠ¤ ëª…ìœ¼ë¡œ ë³€ê²½í•˜ëŠ” ì‘ì—…ë§Œ ìˆ˜í–‰í•˜ì…”ë„ ë¬´ë°©í•  ê²ƒ ìœ¼ë¡œ íŒë‹¨ë©ë‹ˆë‹¤. 
ì „ì²´ ìƒíƒœ ë¨¸ì‹  Json íŒŒì¼ : [`Json`](https://www.notion.so/Json-f15c8804eafd430aa3f24f11796ee232?pvs=21)



## 6.1 ìƒíƒœ ë¨¸ì‹  ìƒì„±

- [Stepfunction Console](https://ap-northeast-2.console.aws.amazon.com/states/home?region=ap-northeast-2#/statemachines)

![Untitled]( ../img/Untitled%2067.png)

![Untitled]( ../img/Untitled%2068.png)

## 6.2 Step ìƒì„±


ğŸ’¡ T0 â†’ T1 â†’ T2 ìˆœì„œëŒ€ë¡œ ì‘ì—…ì„ ìˆ˜í–‰
ì‹¤ì œ í˜„ì—…ì—ì„œ ì‚¬ìš©í•˜ë ¤ë©´ íŒŒë¼ë¯¸í„°ë¥¼ í†µí•´ ë‚ ì§œ(ë…„ì›”ì¼)ì„ íŒŒë¼ë¯¸í„°ë¡œ í•˜ì—¬ ì¡°íšŒí•˜ëŠ” ë‚ ì§œë¥¼ ë³€ê²½í•˜ì—¬ ê° Stepì´ ì§„í–‰ë˜ë„ë¡ í•´ì•¼ë˜ë‚˜ ì•„ë˜ ì‘ì—…ì€ Glue Job Scriptì— ì§€ì •ëœ ë‚ ì§œë¡œ ì¡°íšŒë˜ë„ë¡ ì§„í–‰



### 6.2.1 T0 Bronze Flow

1. Parallel ì‘ì—… ì¶”ê°€
2. `StartJobRun` 2ê°œ ì‘ì—… ë³‘ë ¬ë¡œ ì ìš©
    1. ì‘ì—… 1 : T0 Dimension Job
        1. *íƒœìŠ¤íŠ¸ ì™„ë£Œ ëŒ€ê¸°(Glue Job ì´ ëª¨ë‘ ì™„ë£Œ ë ë•Œê¹Œì§€)*
    2. ì‘ì—… 2 : T0 Fact Job
        1. *íƒœìŠ¤íŠ¸ ì™„ë£Œ ëŒ€ê¸°(Glue Job ì´ ëª¨ë‘ ì™„ë£Œ ë ë•Œê¹Œì§€)*
    
    ![Untitled]( ../img/Untitled%2069.png)
    
3. ê° Job ê³¼ ì—°ê²°ë˜ëŠ” Crawler (`StartCrawler`)
    1. cr_de_enhancement_t0_dimension
    2. cr_de_enhancement_t0_fact
    3. íƒœìŠ¤íŠ¸ ì™„ë£Œ ëŒ€ê¸° X
        1. Crawler ì—ì„œëŠ” ì ìš©ì´ ì•ˆë¨.
    
    ![Untitled]( ../img/Untitled%2070.png)
    
4. ê° Crawler ì˜ ìƒíƒœë¥¼ í™•ì¸í•˜ì—¬ ì‘ì—… ì™„ë£Œ ì—¬ë¶€ë¥¼ í™•ì¸í•˜ê³ ì `GetCrawler`ì™€ `Choice`ë¥¼ ì´ìš©
    1. `GetCrawler`
        1. Crawler í˜„ì¬ ì–´ëŠ ìƒíƒœì¸ì§€ í™•ì¸í•˜ëŠ” Step â†’ ê²°ê³¼ë¥¼ ë‹¤ìŒ Job ìœ¼ë¡œ ì „ë‹¬í•˜ê¸° ìœ„í•œ ë‹¨ê³„ì´ê¸°ë„í•¨.
        2. Start Crawler ì™€ ê°™ì€ Nameì„ API íŒŒë¼ë¯¸í„°ë¡œ ì…ë ¥
        - `ê²°ê³¼ë¬¼`
            
            ì•„ë˜ ë‚´ìš©ì´ GetCrawlerë¥¼ í†µí•´ ì „ë‹¬ë˜ëŠ” ê²°ê³¼ë¬¼ì¸ë° "State": "STOPPING" ê°’ì„ ì´ìš©
            
            ```json
            {
            	"Crawler": {
            	      "Classifiers": [],
            	      "CrawlElapsedTime": 49000,
            	      "CreationTime": "2022-08-19T06:46:42Z",
            	      "DatabaseName": "hist-retail",
            	      "LakeFormationConfiguration": {
            	        "AccountId": "",
            	        "UseLakeFormationCredentials": false
            	      },
            	      "LastCrawl": {
            	        "LogGroup": "/aws-glue/crawlers",
            	        "LogStream": "cr_retail_factdata_sales",
            	        "MessagePrefix": "62ef7b9f-e54d-4b54-805c-d29e905ac4e4",
            	        "StartTime": "2022-08-19T08:01:25Z",
            	        "Status": "SUCCEEDED"
            	      },
            	      "LastUpdated": "2022-08-19T07:12:38Z",
            	      "LineageConfiguration": {
            	        "CrawlerLineageSettings": "DISABLE"
            	      },
            	      "Name": "cr_retail_factdata_sales",
            	      "RecrawlPolicy": {
            	        "RecrawlBehavior": "CRAWL_NEW_FOLDERS_ONLY"
            	      },
            	      "Role": "AWSGlueServiceRole-PipelineRole",
            	      "SchemaChangePolicy": {
            	        "DeleteBehavior": "LOG",
            	        "UpdateBehavior": "LOG"
            	      },
            	      "State": "STOPPING", // ë‹¤ìŒ Step Choice Stateì—ì„œ ì‚¬ìš©í•  Keyê°’.
            	      "Targets": {
            	        "CatalogTargets": [],
            	        "DeltaTargets": [],
            	        "DynamoDBTargets": [],
            	        "JdbcTargets": [],
            	        "MongoDBTargets": [],
            	        "S3Targets": [
            	          {
            	            "Exclusions": [
            	              "**/_temporary/**"
            	            ],
            	            "Path": "s3://hist-retail/factdata/sales"
            	          }
            	        ]
            	      },
            	      "Version": 4
            	    }
            }
            ```
            
        
        ![Untitled]( ../img/Untitled%2071.png)
        
    2. `Choice` - 2ê°œ ë™ì¼í•˜ê²Œ ì ìš©
        1. Crawlerì˜ ìƒíƒœì— ë”°ë¼ Waitë¥¼ í•  ê²ƒì¸ì§€ Successë¥¼ ì§„í–‰ í•  ê²ƒì¸ì§€ íŒë‹¨.
            
            ![Untitled]( ../img/Untitled%2072.png)
            
            ![Untitled]( ../img/Untitled%2073.png)
            
        2. Rules ì— ì•„ë˜ì™€ ê°™ì€ ë‚´ìš©ì„ ì¶”ê°€ 
            - ì „ì²´ ì‹
                - $.Crawler.State == "RUNNING"
            - `Variable`
                - $.Crawler.State
            - `Operator`
                - is equal to
            - `Value`
                - String constant
            - `Text`
                - RUNNING
            
            ![Untitled]( ../img/Untitled%2074.png)
            
        3. RUNNING ì—°ê²°ë¶€ë¶„ì—ëŠ” Wait â†’ ë‹¤ìŒ ìƒíƒœë¥¼ í•´ë‹¹ Choiceìœ„ì˜ GetCrawler ë¡œ ì„ íƒ
            1. ì˜ˆ) T0 Dimension Choice â†’ Wait â†’ T0 Dimension GetCrawler
            
            ![Untitled]( ../img/Untitled%2075.png)
            
    
5. ìµœì¢… ê²°ê³¼ë¬¼ ë° ì‹¤í–‰ ê²°ê³¼
    
    ![Untitled]( ../img/Untitled%2076.png)
    
    ![Untitled]( ../img/Untitled%2077.png)
    

### 6.2.2 T1 Silver Flow (Full Join)

Silver Dataì¸ ì „ì²´ ë°ì´í„°ë¥¼ Join í•œ ê²°ê³¼ë¬¼ì„ ë§Œë“œëŠ” ìˆœì„œ

ë°©ë²™ì€ ìœ„ì™€ ë™ì¼í•˜ê²Œ StartJob â†’ Crawler â†’ Check Crawler Status â†’ Next

1. StartJobRun
    1. ì´ë¦„ : jb_de_enhancement_t1_fulljoin_s2s
    2. íƒœìŠ¤í¬ ì™„ë£Œ ëŒ€ê¸° ì„ íƒ
    
    ![Untitled]( ../img/Untitled%2078.png)
    
2. Crawler ì‹¤í–‰ ë° ìƒíƒœ ì²´í¬ 
    1. StartCrawler
        1. ì´ë¦„ : cr_de_enhancement_t1_purchase_all
    2. GetCrawler
        1. ì´ë¦„ : cr_de_enhancement_t1_purchase_all
    3. Choice
        1. Condition : ìœ„ì—ì„œ ì ìš©í–ˆë˜ Choiceì™€ ê°™ìŒ
        2. Default â†’ Gold Data Flowë¡œ ê°€ë„ë¡ ì²˜ë¦¬
    
    ![Untitled]( ../img/Untitled%2079.png)
    

### 6.2.3 T2 Gold Flow(Data Mart)

ì´ 4ê°œì˜ ë§ˆíŠ¸ë¥¼ ìƒì„± â†’ Job 4, Crawler 4ê°œ ìˆ˜í–‰

ë‹¨, ë§ˆì§€ë§‰ ë‹¨ê³„ì´ë¯€ë¡œ Status CheckëŠ” í•˜ì§€ ì•Šê³  ë§ˆë¬´ë¦¬í•¨

1. ë³‘ë ¬ ì²˜ë¦¬ë¥¼ ìœ„í•œ `Parallel` ì¶”ê°€
2. 4ê°œì˜ Glue Job Start ì¶”ê°€ 
    1. ê° Job ì´ë¦„ ì…ë ¥ 
        1. jb_de_enhancement_t2_salesbyageproducts_s2s
        2. jb_de_enhancement_t2_salesbycustomer_s2s
        3. jb_de_enhancement_t2_salesbydatetime_s2s
        4. jb_de_enhancement_t2_salesbyresidence_s2s
    2. ê° íƒœìŠ¤í¬ ì™„ë£Œ ëŒ€ê¸° ì„ íƒ
3. ê°ê° Job ì— ë§ëŠ” Crawlerë¥¼ ë‹¤ìŒ Stepìœ¼ë¡œ ì…ë ¥
    1. ì´ë¦„
        1. cr_de_enhancement_t2_salesbyageproducts
        2. cr_de_enhancement_t2_salesbycustomer
        3. cr_de_enhancement_t2_salesbydatetime
        4. cr_de_enhancement_t2_salesbyresidence

![Untitled]( ../img/Untitled%2080.png)

### 6.2.4 ìµœì¢… ì „ì²´ Step

1. ì†Œìš” ì‹œê°„
    - ì´ˆê¸° ì ì¬ : 20ë¶„
    - ì¦ë¶„ ì ì¬ : 10ë¶„
2. ì „ì²´ Json File
    - `Json`
        
        ```json
        {
          "Comment": "A description of my state machine",
          "StartAt": "T0 : Parallel",
          "States": {
            "T0 : Parallel": {
              "Type": "Parallel",
              "Branches": [
                {
                  "StartAt": "T0 Dimension Start Job",
                  "States": {
                    "T0 Dimension Start Job": {
                      "Type": "Task",
                      "Resource": "arn:aws:states:::glue:startJobRun.sync",
                      "Parameters": {
                        "JobName": "jb_de_enhancement_t0_dimension_d2s"
                      },
                      "Next": "T0 Dimension StartCrawler"
                    },
                    "T0 Dimension StartCrawler": {
                      "Type": "Task",
                      "Parameters": {
                        "Name": "cr_de_enhancement_t0_dimension"
                      },
                      "Resource": "arn:aws:states:::aws-sdk:glue:startCrawler",
                      "Next": "T0 Dimension GetCrawler"
                    },
                    "T0 Dimension GetCrawler": {
                      "Type": "Task",
                      "Parameters": {
                        "Name": "cr_de_enhancement_t0_dimension"
                      },
                      "Resource": "arn:aws:states:::aws-sdk:glue:getCrawler",
                      "Next": "T0 Dimension Choice"
                    },
                    "T0 Dimension Choice": {
                      "Type": "Choice",
                      "Choices": [
                        {
                          "Variable": "$.Crawler.State",
                          "StringEquals": "RUNNING",
                          "Next": "Wait T0 Dimension"
                        }
                      ],
                      "Default": "Success T0 Dimension"
                    },
                    "Wait T0 Dimension": {
                      "Type": "Wait",
                      "Seconds": 5,
                      "Next": "T0 Dimension GetCrawler"
                    },
                    "Success T0 Dimension": {
                      "Type": "Succeed"
                    }
                  }
                },
                {
                  "StartAt": "T0 Fact Start Job",
                  "States": {
                    "T0 Fact Start Job": {
                      "Type": "Task",
                      "Resource": "arn:aws:states:::glue:startJobRun.sync",
                      "Parameters": {
                        "JobName": "jb_de_enhancement_t0_fact_d2s"
                      },
                      "Next": "T0 Fact StartCrawler"
                    },
                    "T0 Fact StartCrawler": {
                      "Type": "Task",
                      "Parameters": {
                        "Name": "cr_de_enhancement_t0_fact"
                      },
                      "Resource": "arn:aws:states:::aws-sdk:glue:startCrawler",
                      "Next": "T0 Fact GetCrawler"
                    },
                    "T0 Fact GetCrawler": {
                      "Type": "Task",
                      "Parameters": {
                        "Name": "cr_de_enhancement_t0_fact"
                      },
                      "Resource": "arn:aws:states:::aws-sdk:glue:getCrawler",
                      "Next": "T0 Fact Choice"
                    },
                    "T0 Fact Choice": {
                      "Type": "Choice",
                      "Choices": [
                        {
                          "Variable": "$.Crawler.State",
                          "StringEquals": "RUNNING",
                          "Next": "Wait T0 Fact"
                        }
                      ],
                      "Default": "Success T1 Dimension"
                    },
                    "Wait T0 Fact": {
                      "Type": "Wait",
                      "Seconds": 5,
                      "Next": "T0 Fact GetCrawler"
                    },
                    "Success T1 Dimension": {
                      "Type": "Succeed"
                    }
                  }
                }
              ],
              "Next": "T1 Silver Start Job"
            },
            "T1 Silver Start Job": {
              "Type": "Task",
              "Resource": "arn:aws:states:::glue:startJobRun.sync",
              "Parameters": {
                "JobName": "jb_de_enhancement_t1_fulljoin_s2s"
              },
              "Next": "T1 Silver StartCrawler"
            },
            "T1 Silver StartCrawler": {
              "Type": "Task",
              "Parameters": {
                "Name": "cr_de_enhancement_t1_purchase_all"
              },
              "Resource": "arn:aws:states:::aws-sdk:glue:startCrawler",
              "Next": "T1 Silver GetCrawler"
            },
            "T1 Silver GetCrawler": {
              "Type": "Task",
              "Parameters": {
                "Name": "cr_de_enhancement_t1_purchase_all"
              },
              "Resource": "arn:aws:states:::aws-sdk:glue:getCrawler",
              "Next": "T1 Silver Choice"
            },
            "T1 Silver Choice": {
              "Type": "Choice",
              "Choices": [
                {
                  "Variable": "$.Crawler.State",
                  "StringEquals": "RUNNING",
                  "Next": "Wait T1 Silver"
                }
              ],
              "Default": "T2 Parallel"
            },
            "Wait T1 Silver": {
              "Type": "Wait",
              "Seconds": 5,
              "Next": "T1 Silver GetCrawler"
            },
            "T2 Parallel": {
              "Type": "Parallel",
              "End": true,
              "Branches": [
                {
                  "StartAt": "T2 Mart1 Start Job",
                  "States": {
                    "T2 Mart1 Start Job": {
                      "Type": "Task",
                      "Resource": "arn:aws:states:::glue:startJobRun.sync",
                      "Parameters": {
                        "JobName": "jb_de_enhancement_t2_salesbyageproducts_s2s"
                      },
                      "Next": "T2 Mart1 StartCrawler"
                    },
                    "T2 Mart1 StartCrawler": {
                      "Type": "Task",
                      "End": true,
                      "Parameters": {
                        "Name": "cr_de_enhancement_t2_salesbyageproducts"
                      },
                      "Resource": "arn:aws:states:::aws-sdk:glue:startCrawler"
                    }
                  }
                },
                {
                  "StartAt": "T2 Mart2 Start Job",
                  "States": {
                    "T2 Mart2 Start Job": {
                      "Type": "Task",
                      "Resource": "arn:aws:states:::glue:startJobRun.sync",
                      "Parameters": {
                        "JobName": "jb_de_enhancement_t2_salesbycustomer_s2s"
                      },
                      "Next": "T2 Mart2 StartCrawler"
                    },
                    "T2 Mart2 StartCrawler": {
                      "Type": "Task",
                      "End": true,
                      "Parameters": {
                        "Name": "cr_de_enhancement_t2_salesbycustomer"
                      },
                      "Resource": "arn:aws:states:::aws-sdk:glue:startCrawler"
                    }
                  }
                },
                {
                  "StartAt": "T2 Mart3 Start Job",
                  "States": {
                    "T2 Mart3 Start Job": {
                      "Type": "Task",
                      "Resource": "arn:aws:states:::glue:startJobRun.sync",
                      "Parameters": {
                        "JobName": "jb_de_enhancement_t2_salesbydatetime_s2s"
                      },
                      "Next": "T2 Mart3 StartCrawler"
                    },
                    "T2 Mart3 StartCrawler": {
                      "Type": "Task",
                      "End": true,
                      "Parameters": {
                        "Name": "cr_de_enhancement_t2_salesbydatetime"
                      },
                      "Resource": "arn:aws:states:::aws-sdk:glue:startCrawler"
                    }
                  }
                },
                {
                  "StartAt": "T2 Mart4 Start Job",
                  "States": {
                    "T2 Mart4 Start Job": {
                      "Type": "Task",
                      "Resource": "arn:aws:states:::glue:startJobRun.sync",
                      "Parameters": {
                        "JobName": "jb_de_enhancement_t2_salesbyresidence_s2s"
                      },
                      "Next": "T2 Mart4 StartCrawler"
                    },
                    "T2 Mart4 StartCrawler": {
                      "Type": "Task",
                      "End": true,
                      "Parameters": {
                        "Name": "cr_de_enhancement_t2_salesbyresidence"
                      },
                      "Resource": "arn:aws:states:::aws-sdk:glue:startCrawler"
                    }
                  }
                }
              ]
            }
          }
        }
        ```
        

![Untitled]( ../img/Untitled%2081.png)

![Untitled]( ../img/Untitled%2082.png)

![Untitled]( ../img/Untitled%2083.png)

![Untitled]( ../img/Untitled%2084.png)

# 7. Amazon EventBridge



ë°ì´í„°ë¥¼ ì ì¬í•´ì•¼í•˜ëŠ” ë§¤ ìˆœê°„ë§ˆë‹¤ í•´ë‹¹ `AWS Stepfunction`ì˜ ìƒíƒœ ë¨¸ì‹ ì„ êµ¬ë™ì‹œí‚¤ëŠ”ê²ƒì€ ë°˜ë³µë˜ëŠ” ì—”ì§€ë‹ˆì–´ì˜ ì—…ë¬´ ê³¼ì¤‘ ë° ë‚­ë¹„ê°€ ë˜ë¯€ë¡œ í•´ë‹¹ ì‘ì—…ì´ íŠ¹ì • ì¡°ê±´(ë‚ ì§œ)ì— ë§ì¶°ì„œ êµ¬ë™ë˜ë„ë¡ ìŠ¤ì¼€ì¤„ì„ ì ìš©í•  ìˆ˜ ìˆë‹¤.

EventBridgeì˜ TimeBase Ruleì„ í†µí•´ ì‘ì—… ë“±ë¡ ìˆ˜í–‰

![Untitled]( ../img/Untitled%2085.png)

![Untitled]( ../img/Untitled%2086.png)

- ê·œì¹™ : 0 16 15 * ? *  â‡’ Cron Tab ìŠ¤ì¼€ì¤„ë§ ë°©ë²•ê³¼ ë™ì¼
    - ë§¤ì›” 16ì¼ ì˜¤ì „ 1ì‹œì— ìˆ˜í–‰(í•œêµ­ì‹œê°„ ê¸°ì¤€)

![Untitled]( ../img/Untitled%2087.png)

- ëŒ€ìƒ ì„ íƒ
    - Step Functions ìƒíƒœ ë¨¸ì‹ 
    - ìœ„ì—ì„œ ìƒì„±í•œ ë¨¸ì‹  ì„ íƒ
    - ì‹¤í–‰ ì—­í• 
        - ê·¸ìë¦¬ì—ì„œ ìƒì„±í•´ë„ë˜ê³  ë”°ë¡œ Roleì„ ë§Œë“¤ì–´ë„ë¨.

![Untitled]( ../img/Untitled%2088.png)

![Untitled]( ../img/Untitled%2089.png)

# 8. ì°¸ê³ ìë£Œ



ìœ„ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ Lotte ê³„ì—´ì‚¬ì¸ LOHBâ€™S ë§¤ì¥ì„ ì–´ë””ì— ì‹ ê·œë¡œ ì˜¤í”ˆí•˜ë©´ ê´œì°®ì€ì§€ë¥¼ ë¶„ì„í•´ë³¸ ìë£Œ.

[L.Point (ê²½ì§„ëŒ€íšŒ) ë°ì´í„°ë¥¼ í™œìš©í•œ ì–´ë””ì— ì‹ ê·œ LOHB'S ë§¤ì¥ì„ ì˜¤í”ˆ í•´ì•¼í• ê¹Œ?](https://velog.io/@bjlee0689/L.Point-ê²½ì§„ëŒ€íšŒ-ë°ì´í„°ë¥¼-í™œìš©í•œ-ì–´ë””ì—-ì‹ ê·œ-LOHBS-ë§¤ì¥ì„-ì˜¤í”ˆ-í•´ì•¼í• ê¹Œ)

# ì°¸ì¡°



[ì‹¤ë¬´ìë¥¼ ìœ„í•œ ë°ì´í„° ìƒì‹ A to H](https://support.heartcount.io/blog/data-knowledge-atoh)