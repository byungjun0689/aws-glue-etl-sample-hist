# 5. Glue

ğŸ’¡ [AWS Glueë€?](https://www.notion.so/AWS-Glue-21a4e620cac84c54a1960d5f7d801697?pvs=21)

ğŸ’¡ `ê³µí†µ`
IAM Role : `DE-CF-GlueExecute-Role`
- SecretsManager Read/Write
- S3 Full Access
- Glue Service Role

AWS Glue ì—ì„œ Job, Crawlerë¥¼ ì‹¤í–‰ ì‹œí‚¬ ë•Œ í•„ìš”í•œ Role ì„ ì‚¬ì „ ìƒì„±.
        
## 5.6 [Glue Job] T2, Gold Data


ğŸ’¡ ëŒ€ë¶€ë¶„ì˜ Gold Dataì˜ ê²½ìš° Summaryëœ í˜•íƒœì˜ ë°ì´í„°ë¥¼ ë§ì´ ìƒì„±í•œë‹¤. 
GroupBy, Sum, Avg ë“± BI ë˜ëŠ” ë¦¬í¬íŠ¸ì— ë§ëŠ” í˜•íƒœë¡œ ë°ì´í„°ë¥¼ ë³€í™˜í•˜ì—¬ ì ì¬ ìˆ˜í–‰
í•´ë‹¹ ì‹¤ìŠµì€ ë°ì´í„° ë¶„ì„ì„ ëª©ì ìœ¼ë¡œ í•˜ëŠ”ê²Œ ì•„ë‹ˆë¼ ì—”ì§€ë‹ˆì–´ë§ì„ ëª©ì ìœ¼ë¡œ í•˜ê¸°ì— ê°„ë‹¨í•œ ì§€í‘œë§Œ ìƒì„±í•  ì˜ˆì •

### 5.6.1 ì£¼ì œ

> í•´ë‹¹ ì£¼ì œëŠ” ì„ì˜ë¡œ ì •ì˜í•œ ì£¼ì œë¡œ, ì¶”í›„ ê°œë³„ì ìœ¼ë¡œ ì‹¤ìŠµí•˜ì‹¤ë•ŒëŠ” ë‹¤ë¥´ê²Œ ì£¼ì œë¥¼ ì¡ì•„ì„œ ìˆ˜í–‰í•´ë„ ë©ë‹ˆë‹¤.
> 
1. ê³„ì—´ì‚¬
    1. ë…„, ì›”, ì¼ ë§¤ì¶œ, ê±´ìˆ˜
        - `SQL`
            
            ```sql
            SELECT 
                "affiliate"
                , "purchase_year"
                , "purchase_month"
                , round(sum(amount) / 1000000,2) as sum_of_amount
                , count(amount) as cnt_purchase 
                FROM "AwsDataCatalog"."de-enhancement-db"."purchase_all_info"
            group by "affiliate", "purchase_year", "purchase_month"
            order by affiliate, purchase_year, purchase_month
            ```
            
    2. ìš”ì¼, ì‹œê°„ ë§¤ì¶œ, ê±´ìˆ˜
        - `SQL`
            
            ```sql
            -- Monday 1, Tuesday 2, Wednesday 3, Thursday 4, Friday 5, Saturday 6, Sunday 7
            SELECT 
                "affiliate"
                , date_format(date_parse(purchase_date, '%Y%m%d'), '%W') as day_of_week
                , day_of_week(date_parse(purchase_date, '%Y%m%d')) as day_of_week_num
                , "purchase_time"
                , round(sum(amount) / 1000000,2) as sum_of_amount
                , count(amount) as cnt_purchase 
                FROM "AwsDataCatalog"."de-enhancement-db"."purchase_all_info"
            group by 1,2,3,4
            order by 1,3,4
            ```
            
    3. ì œí’ˆ (ì¹´í…Œê³ ë¦¬) ë³„ ë§¤ì¶œ
        - `SQL`
            
            ```sql
            SELECT 
                "affiliate"
                , "division_cd"
                , "main_category_desc"
                , "sub_category_desc"
                , round(sum(amount) / 1000000,2) as sum_of_amount
                , count(amount) as cnt_purchase 
            FROM "AwsDataCatalog"."de-enhancement-db"."purchase_all_info"
            Group by 1,2,3,4
            order by 1,2,3,4
            ```
            
    4. ì—°ë ¹ëŒ€ë³„, ì œí’ˆ(ì¹´í…Œê³ ë¦¬)ë³„ ë§¤ì¶œ
        - `SQL`
            
            ```sql
            SELECT 
                "affiliate"
                ,"age_group"
                , "division_cd"
                , "main_category_desc"
                , "sub_category_desc"
                , round(sum(amount) / 1000000,2) as sum_of_amount
                , count(amount) as cnt_purchase 
            FROM "AwsDataCatalog"."de-enhancement-db"."purchase_all_info"
            Group by 1,2,3,4,5
            order by 1,2,6 desc
            ```
            
    5. ì§€ì—­ë³„ ë§¤ì¶œ
        - `SQL`
            
            ```sql
            SELECT 
                "affiliate"
                ,"province_city"
                ,"city_county"
                , round(sum(amount) / 1000000,2) as sum_of_amount
                , count(amount) as cnt_purchase 
            FROM "AwsDataCatalog"."de-enhancement-db"."purchase_all_info"
            Group by 1,2,3
            order by 1,4 desc
            ```
            
2. ê³ ê°
    1. ê³„ì—´ì‚¬, ì „ì²´ êµ¬ë§¤ ê±´ìˆ˜, ê³ ê° ìˆ˜, í‰ê·  êµ¬ë§¤ ê¸ˆì•¡, í‰ê·  êµ¬ë§¤ íšŸìˆ˜
        - `SQL`
            
            ```sql
            SELECT 
                "affiliate"
                , count(distinct "customer_id") as number_of_customer
                , count("customer_id") as cnt_of_purchase
                , count("customer_id") / count(distinct "customer_id") as avg_number_of_purchase
                , round(sum("amount") / count("customer_id") / 10000,2) as avg_purchase_amount
                , round(sum("amount") / count(distinct "customer_id") / 10000,2) as avg_purchase_amount_per_customer
            FROM "AwsDataCatalog"."de-enhancement-db"."purchase_all_info"
            Group by 1
            Order by 1
            ```
            
    2. ê³„ì—´ì‚¬, ë…„, ì›” ë³„ êµ¬ë§¤ ê±´ìˆ˜, êµ¬ë§¤ ê¸ˆì•¡
        - `SQL`
            
            ```sql
            SELECT 
                "affiliate"
                , "customer_id"
                , "purchase_year"
                , "purchase_month"
                , count("customer_id") as cnt_of_purchase
                , round(sum("amount") / count("customer_id") / 10000,2) as avg_purchase_amount
            FROM "AwsDataCatalog"."de-enhancement-db"."purchase_all_info"
            Group by 1, 2, 3, 4
            Order by 1, 2, 3, 4
            ```
            

### 5.6.2 Glue Job(4ê°œ)


ğŸ’¡ Glue Job ìƒì„±í•˜ê³  Detail ì†ì„±ë“¤ì€ ìœ„ì— ì‘ì—…í–ˆì—ˆë˜ Glueì™€ ë™ì¼í•˜ê±°ë‚˜ Worker ìˆ˜ë§Œ ì¡°ì ˆí•˜ì—¬ ì§„í–‰ 
ë”°ë¡œ ì‘ì„±í•˜ì§€ ì•Šê² ìŠµë‹ˆë‹¤. 

`ì½”ë“œ ë³€ê²½`
â†’ Output Path



1. ê³„ì—´ì‚¬ë³„ + ë…„ + ì›” + ì¼ + ì‹œê°„ + ìš”ì¼ + ë§¤ì¶œ ê±´ìˆ˜ + ë§¤ì¶œ ê¸ˆì•¡
    - ì´ë¦„ : {blee}_jb_de_enhancement_t2_salesbydatetime_s2s
    - `ë³€ê²½ì´ í•„ìš”í•œ ë¶€ë¶„`
        - Glue Database ëª…
        - Output Path
    - `Script`
        
        ```python
        # # 1. ê³„ì—´ì‚¬ë³„ + ë…„ + ì›” + ì¼ + ì‹œê°„ + ìš”ì¼ + ë§¤ì¶œ ê±´ìˆ˜ + ë§¤ì¶œ ê¸ˆì•¡
        import sys
        from awsglue.transforms import *
        from awsglue.utils import getResolvedOptions
        from pyspark.context import SparkContext
        from awsglue.context import GlueContext
        from awsglue.job import Job
        from pyspark.sql.functions import col, substring, count, sum, udf
        
        from datetime import datetime
        import json
        import boto3
        from botocore.exceptions import ClientError
        
        # @params: [JOB_NAME]
        args = getResolvedOptions(sys.argv, ['JOB_NAME'])
        
        sc = SparkContext()
        glueContext = GlueContext(sc)
        spark = glueContext.spark_session
        
        # --
        # -- Overwrite setting
        # --
        spark.conf.set("spark.sql.sources.partitionOverwriteMode","dynamic")  #  ì—†ìœ¼ë©´ ì „ì²´ Partitionì´ overwrite ëœë‹¤ 
        
        job = Job(glueContext)
        job.init(args['JOB_NAME'], args)
        
        hadoop_conf = glueContext._jsc.hadoopConfiguration()
        hadoop_conf.set("mapreduce.fileoutputcommitter.marksuccessfuljobs", "false")  # SUCCESS í´ë” ìƒì„± ë°©ì§€
        hadoop_conf.set("fs.s3.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem")  # $folder$ í´ë”  ìƒì„± ë°©ì§€ 
        
        glue_database_name = "de-enhancement-db"
        
        # --
        # -- FACT : purchase_all_info
        # --
        
        purchase_year = '2014'
        purchase_from_month = '01'
        purchase_to_month = '12'
        
        push_down_predicate = f"purchase_year={purchase_year} and purchase_month between {purchase_from_month} and {purchase_to_month}"
        
        purchase_all_info_dynamic_frame = glueContext.create_dynamic_frame.from_catalog(database = glue_database_name, 
                                                                            table_name = 'purchase_all_info',
                                                                            push_down_predicate = f"({push_down_predicate})")
        
        purchase_all_info_df = purchase_all_info_dynamic_frame.toDF()
        purchase_all_info_df = purchase_all_info_df.withColumn("purchase_day", substring(col("purchase_date"),7,2))
        
        purchase_all_info_df.cache()
        
        purchase_all_info_df.show(3)
        
        ## ê³„ì—´ì‚¬, ë…„, ì›”, ì¼, ì‹œê°„ + ë§¤ì¶œ ê±´ìˆ˜, ë§¤ì¶œ ê¸ˆì•¡ 
        
        grouped_purchase_df = purchase_all_info_df.groupBy(['affiliate', 'purchase_year', 'purchase_month', 'purchase_day', 'purchase_time'])\
                                                    .agg(sum("amount").alias("total_purchase_amount"), count("amount").alias("count_of_purchase"))
        
        ## Group By  ê²°ê³¼ë¬¼ì— ìš”ì¼ ì¶”ê°€ 
        def change_day_of_week(year: str, month: str, day: str):
            try:
                date_string = f"{year}{month}{day}"
                # Convert the date string to a datetime object
                date_object = datetime.strptime(date_string, '%Y%m%d')
                # Get the day of the week as an integer (0 = Monday, 6 = Sunday)
                day_of_week = date_object.weekday()
                # Convert the integer to the corresponding day name
                day_name = date_object.strftime('%A')
                return day_name
            except ValueError:
                return "Invalid date format. Please"
        
        #print(change_day_of_week('2014','11','12') == 'Wednesday')
        
        change_day_of_week_udf = udf(change_day_of_week)
        
        grouped_purchase_df = grouped_purchase_df.withColumn("day_of_week", change_day_of_week_udf(col("purchase_year"), col("purchase_month"), col("purchase_day")))
        
        grouped_purchase_df = grouped_purchase_df.select('affiliate','purchase_year','purchase_month','purchase_day','day_of_week','purchase_time','total_purchase_amount','count_of_purchase')
        
        output_path = output_path = f's3://blee-lab/glue/data/fact/gold/mart_salesbydatetime/'
        grouped_purchase_df.coalesce(1).write.mode('overwrite').partitionBy(['affiliate','purchase_year','purchase_month']).parquet(output_path)
        
        job.commit()
        ```
        
    
    ![Untitled]( ../img/Untitled%2056.png)
    
    ![Untitled]( ../img/Untitled%2057.png)
    
2. ê³„ì—´ì‚¬ë³„ + ì—°ë ¹ë³„ + ì œí’ˆ(ì¹´í…Œê³ ë¦¬)ë³„ ë§¤ì¶œ
    - ì´ë¦„ : {blee}_jb_de_enhancement_t2_salesbyageproducts_s2s
    - `ë³€ê²½ì´ í•„ìš”í•œ ë¶€ë¶„`
        - Glue Database ëª…
        - Output Path
    - `Script`
        
        ```python
        # # 2. ê³„ì—´ì‚¬ë³„ + ì—°ë ¹ë³„ + ì œí’ˆ(ì¹´í…Œê³ ë¦¬)ë³„ ë§¤ì¶œ
        import sys
        from awsglue.transforms import *
        from awsglue.utils import getResolvedOptions
        from pyspark.context import SparkContext
        from awsglue.context import GlueContext
        from awsglue.job import Job
        from pyspark.sql.functions import col, substring, count, sum
        
        import json
        import boto3
        from botocore.exceptions import ClientError
        
        ## @params: [JOB_NAME]
        args = getResolvedOptions(sys.argv, ['JOB_NAME'])
        
        sc = SparkContext()
        glueContext = GlueContext(sc)
        spark = glueContext.spark_session
        
        # --
        # -- Overwrite setting
        # --
        spark.conf.set("spark.sql.sources.partitionOverwriteMode","dynamic")  #  ì—†ìœ¼ë©´ ì „ì²´ Partitionì´ overwrite ëœë‹¤ 
        
        job = Job(glueContext)
        job.init(args['JOB_NAME'], args)
        
        hadoop_conf = glueContext._jsc.hadoopConfiguration()
        hadoop_conf.set("mapreduce.fileoutputcommitter.marksuccessfuljobs", "false")  # SUCCESS í´ë” ìƒì„± ë°©ì§€
        hadoop_conf.set("fs.s3.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem")  # $folder$ í´ë”  ìƒì„± ë°©ì§€ 
        
        glue_database_name = "de-enhancement-db"
        
        # --
        # -- FACT : purchase_all_info
        # --
        
        purchase_year = '2014'
        purchase_from_month = '01'
        purchase_to_month = '12'
        
        push_down_predicate = f"purchase_year={purchase_year} and purchase_month between {purchase_from_month} and {purchase_to_month}"
        
        purchase_all_info_dynamic_frame = glueContext.create_dynamic_frame.from_catalog(database = glue_database_name, 
                                                                            table_name = 'purchase_all_info',
                                                                            push_down_predicate = f"({push_down_predicate})")
        
        purchase_all_info_df = purchase_all_info_dynamic_frame.toDF()
        
        purchase_all_info_df.cache()
        purchase_all_info_df.show(3)
        
        ## ê³„ì—´ì‚¬ë³„ + ì—°ë ¹ë³„ + ì œí’ˆ(ì¹´í…Œê³ ë¦¬)ë³„ + ë§¤ì¶œ ê±´ìˆ˜, ë§¤ì¶œ ê¸ˆì•¡ 
        grouped_purchase_df = purchase_all_info_df.groupBy(['affiliate', 'purchase_year', 'purchase_month', 'age_group', 'division_cd', 'main_category_desc', 'sub_category_desc'])\
                                                    .agg(sum("amount").alias("total_purchase_amount"), count("amount").alias("count_of_purchase"))
        
        grouped_purchase_df.show(3)
        
        output_path = output_path = f's3://blee-lab/glue/data/fact/gold/mart_salesbyageproducts/'
        grouped_purchase_df.coalesce(1).write.mode('overwrite').partitionBy(['affiliate','purchase_year','purchase_month']).parquet(output_path)
        
        job.commit()
        ```
        
    
    ![Untitled]( ../img/Untitled%2058.png)
    
    ![Untitled]( ../img/Untitled%2059.png)
    
3. ê³„ì—´ì‚¬ë³„ + ì§€ì—­ë³„ + ë§¤ì¶œ
    - ì´ë¦„ : {blee}_jb_de_enhancement_t2_salesbyresidence_s2s
    - `ë³€ê²½ì´ í•„ìš”í•œ ë¶€ë¶„`
        - Glue Database ëª…
        - Output Path
    - `Script`
        
        ```python
        # # ê³„ì—´ì‚¬ë³„, ì§€ì—­ë³„, ë§¤ì¶œ
        
        import sys
        from awsglue.transforms import *
        from awsglue.utils import getResolvedOptions
        from pyspark.context import SparkContext
        from awsglue.context import GlueContext
        from awsglue.job import Job
        from pyspark.sql.functions import col, substring, count, sum
        
        import json
        import boto3
        from botocore.exceptions import ClientError
        
        ## @params: [JOB_NAME]
        args = getResolvedOptions(sys.argv, ['JOB_NAME'])
        
        sc = SparkContext()
        glueContext = GlueContext(sc)
        spark = glueContext.spark_session
        
        # --
        # -- Overwrite setting
        # --
        spark.conf.set("spark.sql.sources.partitionOverwriteMode","dynamic")  #  ì—†ìœ¼ë©´ ì „ì²´ Partitionì´ overwrite ëœë‹¤ 
        
        job = Job(glueContext)
        job.init(args['JOB_NAME'], args)
        
        hadoop_conf = glueContext._jsc.hadoopConfiguration()
        hadoop_conf.set("mapreduce.fileoutputcommitter.marksuccessfuljobs", "false")  # SUCCESS í´ë” ìƒì„± ë°©ì§€
        hadoop_conf.set("fs.s3.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem")  # $folder$ í´ë”  ìƒì„± ë°©ì§€ 
        
        glue_database_name = "de-enhancement-db"
        
        # --
        # -- FACT : purchase_all_info
        # --
        
        purchase_year = '2014'
        purchase_from_month = '01'
        purchase_to_month = '12'
        
        push_down_predicate = f"purchase_year={purchase_year} and purchase_month between {purchase_from_month} and {purchase_to_month}"
        
        purchase_all_info_dynamic_frame = glueContext.create_dynamic_frame.from_catalog(database = glue_database_name, 
                                                                            table_name = 'purchase_all_info',
                                                                            push_down_predicate = f"({push_down_predicate})")
        
        purchase_all_info_df = purchase_all_info_dynamic_frame.toDF()
        purchase_all_info_df.cache()
        
        purchase_all_info_df.show(3)
        
        residence_grouped_purchase_df = purchase_all_info_df.groupBy(['affiliate', 'purchase_year', 'purchase_month','province_city', 'city_county'])\
                                                    .agg(sum("amount").alias("total_purchase_amount"), count("amount").alias("count_of_purchase"))\
                                                    .sort(col("affiliate"), col('purchase_year'), col('purchase_month'), col('total_purchase_amount').desc(), col('count_of_purchase').desc())
        
        residence_grouped_purchase_df.show(3)
        
        # month ê¹Œì§€ partitionì„ ë‚˜ëˆ„ê²Œ ë˜ë©´ 1ê°œ íŒŒì¼ì´ 2.4k ë¡œ ë˜ì„œ ì˜¤íˆë ¤ Read/Write Overheadê°€ ë°œìƒ
        # ì‚¬ì‹¤ìƒ í˜„ì¬ íŒŒì¼ í¬ê¸°ë¡œ ë´¤ì„ë–ˆ ê·¸ëƒ¥ 1ê°œ íŒŒì¼ë¡œ ë§Œë“¤ì–´ë„ë˜ë‚˜ ì¶”í›„ ë°ì´í„°ê°€ ëŠ˜ì–´ë‚œë‹¤ëŠ” ì „ì œí•˜ì— Partition Split
        output_path = output_path = f's3://blee-lab/glue/data/fact/gold/mart_salesbyresidence/'
        residence_grouped_purchase_df.coalesce(1).write.mode('overwrite').partitionBy(['affiliate','purchase_year','purchase_month']).parquet(output_path)
        
        job.commit()
        ```
        
    
    ![Untitled]( ../img/Untitled%2060.png)
    
    ![Untitled]( ../img/Untitled%2061.png)
    
4. ê³ ê°ë³„ + ê° ê³„ì—´ì‚¬ + ë…„ + ì›” + êµ¬ë§¤ ê±´ìˆ˜ + êµ¬ë§¤ ê¸ˆì•¡
    - ì´ë¦„ :  {blee}_jb_de_enhancement_t2_salesbycustomer_s2s
    - `ë³€ê²½ì´ í•„ìš”í•œ ë¶€ë¶„`
        - Glue Database ëª…
        - Output Path
    - `Script`
        
        ```python
        # # 4. ê³„ì—´ì‚¬ë³„ + ì—°ë ¹ë³„ + ì œí’ˆ(ì¹´í…Œê³ ë¦¬)ë³„ ë§¤ì¶œ
        import sys
        from awsglue.transforms import *
        from awsglue.utils import getResolvedOptions
        from pyspark.context import SparkContext
        from awsglue.context import GlueContext
        from awsglue.job import Job
        from pyspark.sql.functions import col, substring, count, sum
        
        import json
        import boto3
        from botocore.exceptions import ClientError
        
        ## @params: [JOB_NAME]
        args = getResolvedOptions(sys.argv, ['JOB_NAME'])
        
        sc = SparkContext()
        glueContext = GlueContext(sc)
        spark = glueContext.spark_session
        
        # --
        # -- Overwrite setting
        # --
        spark.conf.set("spark.sql.sources.partitionOverwriteMode","dynamic")  #  ì—†ìœ¼ë©´ ì „ì²´ Partitionì´ overwrite ëœë‹¤ 
        
        job = Job(glueContext)
        job.init(args['JOB_NAME'], args)
        
        hadoop_conf = glueContext._jsc.hadoopConfiguration()
        hadoop_conf.set("mapreduce.fileoutputcommitter.marksuccessfuljobs", "false")  # SUCCESS í´ë” ìƒì„± ë°©ì§€
        hadoop_conf.set("fs.s3.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem")  # $folder$ í´ë”  ìƒì„± ë°©ì§€ 
        
        glue_database_name = "de-enhancement-db"
        
        # --
        # -- FACT : purchase
        # --
        
        purchase_year = '2014'
        purchase_from_month = '01'
        purchase_to_month = '12'
        
        push_down_predicate = f"purchase_year={purchase_year} and purchase_month between {purchase_from_month} and {purchase_to_month}"
        
        purchase_dynamic_frame = glueContext.create_dynamic_frame.from_catalog(database = glue_database_name, 
                                                                            table_name = 'purchase',
                                                                            push_down_predicate = f"({push_down_predicate})")
        
        purchase_df = purchase_dynamic_frame.toDF()
        
        purchase_df.cache()
        purchase_df.show(3)
        
        grouped_purchase_df = purchase_df.groupBy(['customer_id', 'affiliate', 'purchase_year','purchase_month'])\
                    .agg(sum("amount").alias("total_purchase_amount"), count("amount").alias("count_of_purchase"))
        
        grouped_purchase_df.show(3)
        
        # ## Customer + Zipcode Info Join
        
        # glue catalogì—ì„œ glue dynamic frameìœ¼ë¡œ ë°ì´í„°ë¥¼ ì½ì€ í›„ Spark DataFrameìœ¼ë¡œ ë³€í™˜
        customer_dynamic_frame = glueContext.create_dynamic_frame.from_catalog(database=glue_database_name
                                                                    , table_name = 'customer')
        customer_df = customer_dynamic_frame.toDF()
        
        zipcode_dynamic_frame = glueContext.create_dynamic_frame.from_catalog(database=glue_database_name
                                                                    , table_name = 'zipcode')
        zipcode_df = zipcode_dynamic_frame.toDF()
        
        # Customer + Zipcode Jon
        
        customer_with_zipcode_df = customer_df.join(zipcode_df, customer_df['residence'] == zipcode_df['short_zipcode'], "left")\
                                                .drop(zipcode_df.short_zipcode)
        
        grouped_purchase_full_df = grouped_purchase_df.join(customer_with_zipcode_df
                                                , grouped_purchase_df['customer_id'] == customer_with_zipcode_df['customer_id']
                                                , "left")\
                                                .drop(customer_with_zipcode_df.customer_id)
        
        grouped_purchase_full_df.show(3)
        
        output_path = output_path = f's3://blee-lab/glue/data/fact/gold/mart_salesbycustomer/'
        grouped_purchase_full_df.coalesce(1).write.mode('overwrite').partitionBy(['affiliate','purchase_year','purchase_month']).parquet(output_path)
        
        job.commit()
        ```
        
    
    ![Untitled]( ../img/Untitled%2062.png)
    
    ![Untitled]( ../img/Untitled%2063.png)
    

### 5.6.3 Crawler(4ê°œ)

ìœ„ ê° ì£¼ì œì— ë§ê²Œ S3 Pathë§Œ ì§€ì •í•˜ì—¬ 4ê°œ í¬ë¡¤ëŸ¬ ìƒì„± 

1. ê³„ì—´ì‚¬ë³„ + ë…„ + ì›” + ì¼ + ì‹œê°„ + ìš”ì¼ + ë§¤ì¶œ ê±´ìˆ˜ + ë§¤ì¶œ ê¸ˆì•¡
    - ì´ë¦„ : {blee}_cr_de_enhancement_t2_salesbydatetime
2. ê³„ì—´ì‚¬ë³„ + ì—°ë ¹ë³„ + ì œí’ˆ(ì¹´í…Œê³ ë¦¬)ë³„ ë§¤ì¶œ
    1. ì´ë¦„ : {blee}_cr_de_enhancement_t2_salesbyageproducts
3. ê³„ì—´ì‚¬ë³„ + ì§€ì—­ë³„ + ë§¤ì¶œ
    1. ì´ë¦„ : {blee}_cr_de_enhancement_t2_salesbyresidence
4. ê³ ê°ë³„ + ê° ê³„ì—´ì‚¬ + ë…„ + ì›” + êµ¬ë§¤ ê±´ìˆ˜ + êµ¬ë§¤ ê¸ˆì•¡
    1. ì´ë¦„ : {blee}_cr_de_enhancement_t2_salesbycustomer

![Untitled]( ../img/Untitled%2064.png)

![Untitled]( ../img/Untitled%2065.png)

![Untitled]( ../img/Untitled%2066.png)